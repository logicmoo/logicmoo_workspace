#!/usr/bin/perl -w

## $Revision: 1.166 $


=head1 NAME

TheoryLearner.pl (Script trying to solve multiple problems in large theory by learning from successes)

=head1 SYNOPSIS

TheoryLearner.pl [options] filestem

time ./TheoryLearner.pl --fileprefix='chainy_lemma1/' --filepostfix='.ren' chainy1 | tee chainy1.log

 Options:
   --commonfile=<arg>,      -c<arg>
   --problemsfile=<arg>     -o<arg>
   --uniquify=<arg>,        -F<arg>
   --fileprefix=<arg>,      -e<arg>
   --filepostfix=<arg>,     -s<arg>
   --tmpdir=<arg>,          -T<arg>
   --maxcpulimit=<arg>,     -C<arg>
   --mincpulimit=<arg>,     -U<arg>
   --maxaxiomlimit=<arg>,   -A<arg>
   --permutetimelimit=<arg>, -P<arg>
   --dofull=<arg>,          -f<arg>
   --iterrecover=<arg>,     -I<arg>
   --loadprovedby=<arg>,    -B<arg>
   --runeprover=<arg>,      -E<arg>
   --runspass=<arg>,        -S<arg>
   --runvampire=<arg>,      -V<arg>
   --runparadox=<arg>,      -p<arg>
   --runmace=<arg>,         -M<arg>
   --maceemul=<arg>,        -l<arg>
   --usemodels=<arg>,       -D<arg>
   --incrmodels=<arg>,      -N<arg>
   --srassemul=<arg>,       -R<arg>
   --countersatcheck=<arg>, -k<arg>
   --similarity=<arg>,      -i<arg>
   --generalize=<arg>,      -g<arg>
   --parallelize=<arg>,     -j<arg>
   --iterpolicy=<arg>,      -y<arg>
   --learnpolicy=<arg>,     -O<arg>
   --iterlimit=<arg>,       -t<arg>
   --recadvice=<arg>,       -a<arg>
   --snowserver=<arg>,      -W<arg>
   --reuseeval=<arg>,       -u<arg>
   --limittargets=<arg>,    -L<arg>
   --boostlimit=<arg>,      -b<arg>
   --boostweight=<arg>,     -w<arg>
   --refsbgcheat=<arg>,     -r<arg>
   --alwaysmizrefs=<arg>,   -m<arg>
   --tptpproofs=<arg>,      -z<arg>
   --cache=<arg>,           -H<arg>
   --runepar=<arg>
   --dummy=<arg>           
   --help,                  -h
   --man

=head1 OPTIONS

=over 8

=item B<<< --commonfile=<arg>, -c<arg> >>>

One common file containing all axioms and conjectures that should be proved 
from them. If an extra axiom is needed in this input scenario for a conjecture,
it has to be added as an antecedent of an implication. The --fileprefix
option is mandatory in this case, specifying the directory for autogenerated
problem files.

=item B<<< --problemsfile=<arg>, -o<arg> >>>

One file containing pairs of absolute problem file names and
absolute output file names for solutions. This is the setup
used in CASC LTB division.
The --fileprefix option is mandatory in this case, specifying the working
directory. The --uniquify option is likely to be needed too, at least for
renaming file names and conjectures.

=item B<<< --uniquify=<arg>, -B<F><arg> >>>

If > 0, the formula names in problems are not assumed to be consistent, and
the naming of conjectures does not have to correspond to the problem names.
This means that problem names will be changed, so that conjecture names
could be derived from them, different formulas with the same name will be
disjointly renamed, and same formulas with different names will be jointly
renamed in all problems (with the exception of the same conjecture shared
by two different problems).
The name mappings are remembered, i.e., a mapping of problem names is kept,
and for each problem, the original names of its formulas are kept.
This can be used to observe the CASC solution format (see also --problemsfile).
The default is 0 (experimental so far).
See uniquify.pl in the script directory for implementation.

=item B<<< --fileprefix=<arg>, -e<arg> >>>

Prefix saying how to create problem file names from conjecture names.
It is prepended to the conjecture name (and can contain directory part ended with /).
If the --commonfile option was used, this tells the directory used for
autogenerated problem files (the directory must not exist).

=item B<<< --filepostfix=<arg>, -s<arg> >>>

Postfix saying how to create problem file names from conjecture names.
It is appended to the conjecture name (typically a file extension).

=item B<<< --tmpdir=<arg>, -B<T><arg> >>>

Directory (slash-ended) for temporary problem and result files.
Defaults to "", which means no usage of any special directory.
Otherwise, the tmpdir/fileprefix directory must not previously exist.
The /tmp/ is usable, however /dev/shm/ or similar memory-based
(tmpfs) location is strongly recommended, because the number of
these files is large. After each pass, these files are tgz-ed
into a .pass_nr file, and deleted.

=item B<<< --maxcpulimit=<arg>, -B<C><arg> >>>

Upper limit to which the CPU time for one ATP attempt is grown
exponentially (multiplied by 4 and starting from 1 second base). 
The default is 16 seconds (should be power of 4)

=item B<<< --mincpulimit=<arg>, -B<U><arg> >>>

Lower limit for the CPU time for one ATP attempt.
The default is 1 second (should be power of 4).

=item B<<< --maxaxiomlimit=<arg>, -B<A><arg> >>>

Upper limit to which the number of axioms used for one ATP attempt is grown
exponentially (multiplied by 2 and starting from 4 axioms).
The default is 128 axioms (should be power of 2).

=item B<<< --permutetimelimit=<arg>, -B<P><arg> >>>

Upper time limit upto which fixing of subsumed specifications is done.
I.e. if a specification suggested by the adviser is subsumed, and
the suggested timelimit is less or equal to permutetimelimit, then
the specification will be fixed by adding additional axioms
in order of their relevance. The fixing method is influnced
by --countersatcheck.
Default is equal to mincpulimit. If 0, no fixing is done.

=item B<<< --dofull=<arg>, -f<arg> >>>

If 1, the first pass is a max-timelimit run on full problems. 
If 2, the first pass is a min-timelimit run on full problems. 
If 0, that pass is omitted, and the symbol-only pass is the first run.
Default is 1.

=item B<<< --iterrecover=<arg>, -B<I><arg> >>>

Instead of starting fresh, assume that iteration passed as arg
was already done. Load the result table and all other needed
tables, and continue with the next iteration.

=item B<<< --loadprovedby=<arg>, -B<B><arg> >>>

Load the initial proved_by table from the given file.
Otherwise, the default initial proved_by table contains for each formulas
the info that it can be proved by itself (this initializes the learning).
If an entry for some formula is missing in the given file,
the default info will be added for it.

=item B<<< --runeprover=<arg>, -B<E><arg> >>>

If >= 1, run E. Default is 1.
If greater than 1, run only in passes where the number
of refs is not greater than this.

=item B<<< --runspass=<arg>, -B<S><arg> >>>

If >= 1, run SPASS. Default is 1.
If greater than 1, run only in passes where the number
of refs is not greater than this.

=item B<<< --runvampire=<arg>, -B<V><arg> >>>

If >= 1, run Vampire. Default is 0.
If greater than 1, run only in passes where the number
of refs is not greater than this.

=item B<<< --runparadox=<arg>, -B<p><arg> >>>

If >= 1, run Paradox. Default is 0.
If greater than 1, run only in passes where the number
of refs is not greater than this. Good nonzero default
is then 128.

=item B<<< --runmace=<arg>, -B<M><arg> >>>

If >= 1, and running Paradox, run also Mace to construct a model.
If greater than 1, run only in passes where the number
of refs is not greater than this.
The model is then used for evaluation of formulas. Default is 64,
because this is constraint by --runparadox anyway.

=item B<<< --maceemul=<arg>, -B<l><arg> >>>

If >= 1, and running Paradox, try to forge a Mace4 model
using the prdxprep.pl and prdx2p9.pl programs on Pradaox output. This
replaces running Mace to create the model, which sometimes fails
(Mace avoids models of cardinality 1).
Default is 0 - is experimental.

=item B<<< --usemodels=<arg>, -B<D><arg> >>>

Use models for learning relevance of formulas. If 1, only negative
models are used, if 2, only positive models are used, if 3,
both positive and negative models are used, if 0, none are used.
Default is 1. This asssumes --runmace=1 or --maceemul=1.

=item B<<< --incrmodels=<arg>, -B<N><arg> >>>

If 1, printing and learning of models is done incrementally,
i.e. a new example is printed for each fla that has a new model
found in the last iteration. If 0, the nonincremental (older)
implementation is used, printing the whole models info
for all iterations as one example for each fla with a model.
Default is 0 (more tested). This assumes --usemodels>0.

=item B<<< --srassemul=<arg>, -B<R><arg> >>>

If 1, and running Mace (i.e. models are used), try to extend problem
specifications using the SRASS algorithm. That is: axioms that were
evaluated as false in some model of the negated conjecture are
greedily added (in order of their relevance) until all such models
are covered (or we run out of falsifying axioms).
Default is 1, because this is constraint by --runmace=1 or --maceemul=1 anyway.

=item B<<< --countersatcheck=<arg>, -k<arg> >>>

Default is 1, which means that if a suggested specification is
a subset of a specification that is already known to be countersatisfiable,
axioms will be greedily added (by relevance) until it is no longer
known to be countersatisfiable.
If 2, and --runmace=1 or --maceemul=1 (i.e. models are used), the
model info will be used to fix countersatisfiable specifications. That is,
if a model was found for a previous countersatisfiable specification S, all formulas
will be evaluated in the model, and usually many more will be known to be true
in the model than just the axioms of the specification S. So instead of extending
just the countersatisfiable specifications, the whole set of true axioms has to be
extended for each model. This is a bit related to --srassemul.
Use --permutetimelimit=0 to switch off fixing of countersatisfiable specs completely.

=item B<<< --similarity=<arg>, -i<arg> >>>

The similarity measure for formulas. If 1, only vector of symbols
is used, if 2, only codes of shared terms are used, if 4, the shared terms
are first normalized by renaming all variables to just one generic variable.
If 8, all substitution-tree nodes for all subterms are used instead of just shared terms.
This can be again combined with 4 (normalization). 
Combinations of these basic methods can be done by summing their codes
(i.e., value 15 would tell to use all of them).
Default is 1 (symbols only).


=item B<<< --generalize=<arg>, -g<arg> >>>

The generalization method for formulas. If 0, no generalization is done.
If 1, new generalized formulas are created by replacing all local constants
in formulas with a new special symbol. The generalized formulas
become learning targets exactly as the original ones, and they are included
into training whenever some corresponding original formula is. Accordingly,
when a generalized fla is recommended by the trained system as an axiom,
the corresponding original flas become recommended.
Default is 0.


=item B<<< --parallelize=<arg>, -j<arg> >>>

If greater than 1, runs problems in parallel, using Makefile with
the -j<arg> option. Currently works only with E.
Default is 1 - no parallelization.

=item B<<< --iterpolicy=<arg>, -y<arg> >>>

Policy for iterations. Default is 0 - the standard learning greedy,
minimal axioms loop. Another implemented option is 1: prefers
to grow the axiomlimit to the maximum regardless of previous
success, and only when it is maximal, it drops to the lowest value.

=item B<<< --learnpolicy=<arg>, -B<O><arg> >>>

Policy for learning. Default is 0 - learning is always done.
If > 1, and --iterpolicy is 1, learning only occurs when the maximum
values of the axiomlimit is reached. This can speed things up
when learning is slow, the price is that the new info is not available
immediatelly.

=item B<<< --iterlimit=<arg>, -t<arg> >>>

Depending on the iterpolicy, this is the maximal/minimal number of
iterations. If iterpolicy is 0, it is maximum, otherwise minimum.
One particular use is to set maxcpulimit to 1, iterpolicy to 1,
and this to e.g. 200. It ensures that 200 iterations with
timilimit 1 will be done repetitively through all axiom thresholds,
even if nothing new was learned (so setting permutetimelimit
to a reasonable value is recommended, as it is the only source of
possible new results). Defaults to 10000 with the standard iterpolicy,
and to 50 with the axiomlimit growth policy.


=item B<<< --recadvice=<arg>, -a<arg> >>>

If nonzero, the axiom advising phase is repeated this many times,
recursively using the recommended axioms to enlarge the set of symbols
for the next advising phase. Default is 0 (no recursion).

=item B<<< --snowserver=<arg>, -B<W><arg> >>>

If nonzero, snow is running as server for iterations > 3,
learning only the new info and not re-learning all things again.
If --usemodels>0, it will set --incrmodels to 1, so that
only the new model info could be used. This should speed things
up with large numbers of targets, however it is not clear if
snow really can learn in the server mode, so the default is 0.

=item B<<< --reuseeval=<arg>, -u<arg> >>>

If nonzero, the evaluation of the learner is stored after each pass
(can be a huge file), and if nothing new was proved in the previous
pass, it is re-used (instead of running the same evaluation).
The current implementation will conflict with the modelinfo features,
so don't use it if usemodels is on.
Default is 0 (no re-use, still experimental).

=item B<<< --limittargets=<arg>, -B<L><arg> >>>

If nonzero, it is the maximum number of targets that the
machine learner prints for further consideration. This can
be a useful speed-up when the number of targets is very high
(say 100000), but we only need a small number of them that
are very likely to be recommended within a much smaller initial
segment. Default is 0 - no limit. A useful limit is 1000.

=item B<<< --boostlimit=<arg>, -b<arg> >>>

If nonzero, the axioms in small specifications are slightly
boosted in learning by exp( -boostweight). A specification is
small, if it has less axioms than (boostlimit/100) * number-of-all-axioms.
Default is 0 (no boosting). A reasonable boosting default is 1,
i.e. with 70000 total axioms in all specs, axioms in those
with less than 700 will be boosted. The idea is that this will
help to focus in the much larger specifications, in the same way
as the info about proof helps.

=item B<<< --boostweight=<arg>, -w<arg> >>>

The weight used for boosting if boostlimit > 0. This
is negated and exponentiated to get the boost factor. The default
is 7 (so the boost factor is exp -7 = ca. 0.01), because this is
constraint by boostlimit anyway.

=item B<<< --refsbgcheat=<arg>, -r<arg> >>>

Tells to cheat by limiting background for problems
whose subproblems (specified in .refspec) are solved.
Useful for reproving. Default is 0 - no cheating.

=item B<<< --alwaysmizrefs=<arg>, -m<arg> >>>

If 1, tells to always include the explicit Mizar references. This
is useful for the bushy problems, where we are reasonably sure
that the explicit Mizar references should be used in the proof,
while we are uncertain about the background formulas.
The explicit references are now recongized by matching the regexp
"^[tldes][0-9]+" (for theorems, top-level lemmas, definitions,
sublemmas, and scheme instances).
If 2, references containing Mizar local constants are always
included. These references are recognized by grepping
for "\bc[0-9]+" in the formula symbols.
If 3, it behaves like 1, but the regexp is "^fact_", which is used for
some Isabelle problems.


=item B<<< --tptpproofs, -z<arg> >>>

If > 0, try to get the TPTP format of all found proofs. This
is now done by rerunning EP on the set of needed references found
by other provers. This is mainly used for CASC LTB*, where the
condition is a TPTP proof with adequate FOF->CNF documentation.
The default is 0.

=item B<<< --cache=<arg>, -B<H><arg> >>>

If nonempty, global persistent cache of results is used. This is a
directory with subdirectories named as sha1 of the concatenated
parameters, in which files are named by their sha1 value, and contain
the result of the run. If the cache value exists for a combination of
parameters and file, it is just cat-ed instead of running the
ATP. This saves a lot of time in experimenting, but it must not be
used in competitions and benchmarks when time is important.

=item B<<< --runepar=<arg> >>>

If nonzero, runs E using a special strategy parallelizer.  Default is
0, the parallelization method and strategies are now hardwired for
CASC and Mizar problems.

=item B<<< --dummy=<arg> >>>

If nonempty, points to a dummy file to be added to the set of
problems, which is ignored when pritning LTB solutions. This can be
used to make available axioms mentioned in external training data.

=item B<<< --help, -h >>>

Print a brief help message and exit.

=item B<<< --man >>>

Print the manual page and exit.

=back

=head1 DESCRIPTION

Josef Urban: MaLARea: a Metasystem for Automated Reasoning in Large Theories.
Proceedings of the CADE-21 Workshop on
Empirically Successful Automated Reasoning in Large Theories
Bremen, Germany, 17th July 2007.
http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-257/05_Urban.pdf .

Urban J., Sutcliffe G., Pudlak P., Vyskocil J. (2007),
MaLARea SG1: Machine Learner for Automated Reasoning with Semantic Guidance,
In Baumgartner P., Armando A., Gilles D.,
Proceedings of the 4th International Joint Conference on Automated Reasoning (Sydney, Australia),
Lecture Notes in Artificial Intelligence (To appear).
http://kti.mff.cuni.cz/~urban/MaLAReaSG1.pdf .

=head1 CONTACT

Josef Urban (firstname dot lastname at gmail dot com)

=head1 COPYRIGHT

Copyright (C) 2007-2012 Josef Urban (firstname dot lastname at gmail dot com)

=head1 LICENCE

This program is free software; you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation; either version 2 of the License, or
(at your option) any later version.

=cut

use strict;
use Pod::Usage;
use Getopt::Long;
use IO::Socket;
use IPC::Open2;
use File::Spec;

my $gsymoffset    = 2000000; # offset at which symbol numbering starts
my $gstdtrmoffset = 4000000; # offset at which standard term numbering starts
my $gnrmtrmoffset = 6000000; # offset at which normalized term numbering starts
my $gnegmodeloffset  = 8000000; # offset at which negative model numbering starts
my $gposmodeloffset  = 10000000; # offset at which positive model numbering starts
my %grefnr;                 # Ref2Nr hash for references
my @gnrref;                 # Nr2Ref array for references

my %gsymarity;              # for each symbol its arity and 'p' or 'f'
my %gsymnr;                 # Sym2Nr hash for symbols
my @gnrsym;                 # Nr2Sym array for symbols - takes gsymoffset into account!

my %gmodnr;                 # for each model (file without .mmodel) its number (without any offset)
my @gnrmod;                 # for each model number: its file, symbols, positive refs, negative refs
my %gsum2model;             # hash of sha sums of existing models
my @gitermods;		    # for each iteration holds $#gnrmod achieved at that iteration;
                            # that is, if $gitermods[n] = $gitermods[n-1], no models were found
                            # in n-th iteration

my @ghistory;               # ghistory[$iter] = [$nrproved, $nrtried, $threshold, $timelimit, $modelsfound, $proved]
                            # keeps the statistics for possible policy adjustments

my %grefsyms;     # Ref2Sym hash for each reference array of its symbols
my %greftrmstd;   # Ref2Sym hash for each reference array of its stdterms (their shared-entry numbers)
my %greftrmnrm;   # Ref2Sym hash for each reference array of its nrmterms (their shared-entry numbers)
my %grefposmods;  # Ref2Sym hash for each ref array of its positive models (without offset)
my %grefnegmods;  # Ref2Sym hash for each ref array of its negative models (without offset)
my %gitrefposmods;# Ref2Sym hash for each ref array of its positive models for current iteration
my %gitrefnegmods;# Ref2Sym hash for each ref array of its negative models for current iteration
my %gspec;        # Ref2Spec hash for each reference hash of its initial references
my %gresults;     # hash of results
my %gsubrefs;     # contains direct lemmas for those proved by mizar_proof,
                  # only if $grefsbgcheat == 0
my %gsuperrefs;   # contains additions to bg inherited from direct lemmas
                  # for those proved by mizar_proof, only if $grefsbgcheat == 0

my %glocal_consts_refs; # contains references containing local constants

my %gltbnames;    # for each problem its LTB input and solution (absolute) paths
my %gorigprobs;   # for each conjecture its original problem name
my %gorigflas;    # for each conjecture the hash of fla names that got renamed
                  # (with the original names as values) (if st. is missing, it was not renamed)
my %gref2fla;     # hash of formula bodies in tptp
my %gref2p9fla;   # hash of formula bodies in ladr
my @ggennr2fla;   # array of generalized formulas
my %ggenfla2nr;   # hash for lookup of genflas' numbers
my %gref2gen;     # contains references with generalized reference
my %ggen2ref;     # for each generalized ref contains a list of corresponding original refs
my $gggnewc = 'gggnewc'; # new symbol for generalizing ocnstants; TODO: should check it's not in spec

my %gatpdata;     # for each ATP a hash of its thresholds and other useful data
my $gsnowport;    # port for snow running as a server of guseserver>0; determined at Learn
my $grunner = 'runwtlimit'; # the default non-cached atp runner

my %gsinerel;     # for each conjecture the allowed refs sorted by sine

my $minthreshold = 4;
my ($gcommonfile,  $gfileprefix,    $gfilepostfix,
    $maxtimelimit, $gdofull,        $giterrecover,
    $gspass,       $gvampire,       $grecadvice,
    $grefsbgcheat, $galwaysmizrefs, $gsimilarity,
    $maxthreshold, $mintimelimit,   $permutetimelimit,
    $gparadox,     $geprover,       $gmace,
    $gtmpdir,      $gsrassemul,     $gusemodels,
    $gparallelize, $gmakefile,      $gloadprovedby,
    $gboostlimit,  $gboostweight,   $greuseeval,
    $giterpolicy,  $ggeneralize,    $glimittargets,
    $gmaceemul,    $gincrmodels,    $giterlimit,
    $guniquify,    $gcountersatcheck, $gproblemsfile,
    $gsnowserver,  $glearnpolicy,   $gtptpproofs,
    $gcache,	   $grunepar,       $gdummy);

my ($help, $man);
my $gtargetsnr = 1233;


Getopt::Long::Configure ("bundling");

GetOptions('commonfile|c=s'    => \$gcommonfile,
	   'problemsfile|o=s'    => \$gproblemsfile,
	   'uniquify|F=i'    => \$guniquify,
	   'fileprefix|e=s'    => \$gfileprefix,
	   'filepostfix|s=s'    => \$gfilepostfix,
	   'tmpdir|T=s'    => \$gtmpdir,
	   'maxcpulimit|C=i'    => \$maxtimelimit,
	   'mincpulimit|U=i'    => \$mintimelimit,
	   'permutetimelimit|P=i'    => \$permutetimelimit,
	   'maxaxiomlimit|A=i'  => \$maxthreshold,
	   'dofull|f=i'    => \$gdofull,
	   'iterrecover|I=i' => \$giterrecover,
	   'loadprovedby|B=s' => \$gloadprovedby,
	   'runeprover|E=i'    => \$geprover,
	   'runspass|S=i'    => \$gspass,
	   'runvampire|V=i'    => \$gvampire,
	   'runparadox|p=i'    => \$gparadox,
	   'runmace|M=i'    => \$gmace,
	   'maceemul|l=i'    => \$gmaceemul,
	   'usemodels|D=i'    => \$gusemodels,
	   'incrmodels|N=i'    => \$gincrmodels,
	   'srassemul|R=i'    => \$gsrassemul,
	   'countersatcheck|k=i'    => \$gcountersatcheck,
	   'similarity|i=i'  => \$gsimilarity,
	   'generalize|g=i'  => \$ggeneralize,
	   'parallelize|j=i'  => \$gparallelize,
	   'iterpolicy|y=i'  => \$giterpolicy,
	   'learnpolicy|O=i'  => \$glearnpolicy,
	   'iterlimit|t=i'  => \$giterlimit,
	   'recadvice|a=i'    => \$grecadvice,
	   'snowserver|W=i'    => \$gsnowserver,
	   'reuseeval|u=i'    => \$greuseeval,
	   'limittargets|L=i'    => \$glimittargets,
	   'boostlimit|b=i'    => \$gboostlimit,
	   'boostweight|w=i'    => \$gboostweight,
	   'refsbgcheat|r=i'    => \$grefsbgcheat,
	   'alwaysmizrefs|m=i'    => \$galwaysmizrefs,
	   'tptpproofs|z=i'             => \$gtptpproofs,
	   'cache|H=s'             => \$gcache,
	   'runepar=i'             => \$grunepar,
	   'dummy=s'             => \$gdummy,
	   'help|h'          => \$help,
	   'man'             => \$man)
    or pod2usage(2);

pod2usage(1) if($help);
pod2usage(-exitstatus => 0, -verbose => 2) if($man);

pod2usage(2) if ($#ARGV != 0);

my $filestem   = shift(@ARGV);

sub min { my ($x,$y) = @_; ($x <= $y)? $x : $y }

# $giterpolicy possible values
sub pol_STD      ()  { 0 }
sub pol_GROWTH   ()  { 1 }

$gdofull = 1 unless(defined($gdofull));
$giterrecover = -1 unless(defined($giterrecover));
$gloadprovedby = "" unless(defined($gloadprovedby));
$geprover = 1 unless(defined($geprover));
$gspass = 1 unless(defined($gspass));
$gvampire = 0 unless(defined($gvampire));
$gparadox = 0 unless(defined($gparadox));
$gmace = 64 unless(defined($gmace));
$gmaceemul = 0 unless(defined($gmaceemul));
$gusemodels = 1 unless(defined($gusemodels));
$gincrmodels = 0 unless(defined($gincrmodels));
$gsrassemul = 1 unless(defined($gsrassemul));
$gcountersatcheck = 1 unless(defined($gcountersatcheck));
$gparallelize = 1 unless(defined($gparallelize));
$giterpolicy = pol_STD unless(defined($giterpolicy));
$glearnpolicy = 0 unless(defined($glearnpolicy));
$giterlimit = 0 unless(defined($giterlimit));
$grecadvice = 0 unless(defined($grecadvice));
$gsnowserver = 0 unless(defined($gsnowserver));
$greuseeval = 0 unless(defined($greuseeval));
$glimittargets = 0 unless(defined($glimittargets));
$gboostlimit = 0 unless(defined($gboostlimit));
$gboostweight = 7 unless(defined($gboostweight));
$grefsbgcheat = 0 unless(defined($grefsbgcheat));
$gsimilarity = 1 unless(defined($gsimilarity));
$ggeneralize = 0 unless(defined($ggeneralize));
$galwaysmizrefs = 0 unless(defined($galwaysmizrefs));
$gtptpproofs = 0 unless(defined($gtptpproofs));
$gcommonfile = "" unless(defined($gcommonfile));
$gproblemsfile = "" unless(defined($gproblemsfile));
$guniquify = 0 unless(defined($guniquify));
$gfileprefix = "" unless(defined($gfileprefix));
$gfilepostfix = "" unless(defined($gfilepostfix));
$gtmpdir = "" unless(defined($gtmpdir));
$maxtimelimit = 64 unless(defined($maxtimelimit));  # should be power of 4
$mintimelimit = 1 unless(defined($mintimelimit));  # should be power of 4
$permutetimelimit = $mintimelimit unless(defined($permutetimelimit));
$maxthreshold = 128 unless(defined($maxthreshold)); # should be power of 2
$gcache = "" unless(defined($gcache));
$grunepar = 0 unless(defined($grunepar));
$gdummy = "" unless(defined($gdummy));

# needed for fast grepping
$ENV{"LANG"}= 'C';

# needed for file inclusion to work, we expect this to be set for LTB
# $ENV{'TPTP'}=


my $gtimelimit = $maxtimelimit;
my $gdotrmstd = $gsimilarity & 2;
my $gdotrmnrm = $gsimilarity & 4;
my $gdosyms = $gsimilarity & 1;
my $gdostreedefs = $gsimilarity & 8;

# TODO: make this a param, now off by default
my $garitize = ''; # "aritize,"

# shared features generating program
my $gshgenfeatureprg = ($gdostreedefs > 0)? "fof2streedefs" : "fofshared";

my $gusenegmodels = $gusemodels & 1;
my $guseposmodels = $gusemodels & 2;

my $gmaxiterlimit = 10000;
my $gminiterlimit = 50;

if($giterlimit > 0)
{
    if($giterpolicy == pol_STD) { $gmaxiterlimit = $giterlimit; }
    else { $gminiterlimit = $giterlimit; }
}

if($gusemodels == 0) { $gincrmodels = 0; }

if($gparallelize > 1) { $gmakefile = 1; } else { $gmakefile = 0; }

$gboostlimit  = $gboostlimit / 100;
$gboostweight = exp (- $gboostweight);

$grunner = 'runwtlcached' unless($gcache eq "");


my $gmizrefsregexp = '^[tldes][0-9]+_';
my $gisarefsregexp = '^fact_';
my $gmconstregexp = '\bc[0-9]+';

my $galwaysrefsregexp = $gmizrefsregexp;

$galwaysrefsregexp = $gisarefsregexp if($galwaysmizrefs == 3);

# Vampire version. We should allow more ATP versions/strategies to be
# run simultaneously, this is a quick hack.

my $gvampire_version = '0.6'; # another option is '9'

# TODO: make this an option
my $gusesinerel = 1;


# list of all handled atps
my @gallatps = ('atp_E','atp_EP','atp_SPASS','atp_VAMPIRE','atp_PARADOX',
		'atp_MACE','atp_RANDOCOP','atp_PROVER9');

InitAtpData();

# at this point $gparadox and $gmace are in {0,1}
$gsrassemul = $gparadox * ($gmace + $gmaceemul) * $gsrassemul;

## only weak countersatcheck if models are not available
if(($gcountersatcheck == 2) &&
   (($gparadox == 0) || (($gmace == 0) && ($gmaceemul == 0))))
{
    $gcountersatcheck = 1;
}

## string used to overcome Snow's I/O buffering;
## it is gueranteed to consist of otherwise unused input features
my $gsnowbuffbogus = MkSnowBuffBogus();

## the number of time $gsnowbuffbogus gets written as a separator/flusher
my $gsnowbogusfactor = 10;

## state var saying how many snow ouputs should be ignored
## (because they come from training or bogus)
my $gskipsnowres = 0;

## pid of the snow run as a server
my $gsnowpid;

sub MkSnowBuffBogus
{
    my $bogusvalue = 100000000;
    my @foo = ();
    foreach my $i (0..100) { push(@foo, $bogusvalue); }
    return join(",", @foo) . ":\n";
}


# change for debug printing
sub WNONE	()  { 0 }
sub WSRASS	()  { 1 }
sub WCSAT	()  { 2 }
sub WSNOW	()  { 4 }
sub WREVAL	()  { 8 }
sub GWATCHED 	()  { WREVAL }

# print @msgs if $flag is in GWATCHED
sub watch
{
    my ($flag, @msgs) = @_;
    print @msgs if(GWATCHED & $flag);
}

# print %gresults before dying if possible
# ###TODO: load model info
local $SIG{__DIE__} = sub { DumpResults(); DumpModelInfo(); };

sub LoadTables
{
    my $i = 0;
    my ($ref,$sym,@syms,$psyms,$fsyms);

    %grefnr = ();
    %gsymnr = ();
    %grefsyms = ();
    %glocal_consts_refs = ();
    @gnrsym = ();
    @gnrref = ();
    @ghistory  = ();

    %gmodnr = ();
    @gnrmod = ();
    @gitermods = ();
    %gsum2model = ();
    %grefposmods = ();
    %grefnegmods = ();

    open(REFNR, "$filestem.refnr") or die "Cannot read refnr file";
    open(SYMNR, "$filestem.symnr") or die "Cannot read symnr file";
    open(REFSYMS, "$filestem.refsyms") or die "Cannot read refsyms file";

    while($_=<REFNR>) { chop; push(@gnrref, $_); $grefnr{$_} = $#gnrref;};
    while($_=<SYMNR>) { chop; push(@gnrsym, $_); $gsymnr{$_} = $gsymoffset + $i++; };
    while($_=<REFSYMS>)
    {
	chop; 
	m/^symbols\( *([a-z0-9A-Z_]+) *, *\[(.*)\] *, *\[(.*)\] *\)\./ 
	    or die "Bad symbols info: $_";
	($ref, $psyms, $fsyms) = ($1, $2, $3);
	my @psyms = split(/\,/, $psyms);
	my @fsyms = split(/\,/, $fsyms);
	my @allsyms = (@psyms, @fsyms);
	$grefsyms{$ref} = [];
	foreach $sym (@allsyms)
	{

	    $sym =~ m/^ *([^\/]+)[\/].*/ or die "Bad symbol $sym in $_";
	    my $proper_sym = $1;
	    push(@{$grefsyms{$ref}}, $proper_sym);
	    if($proper_sym =~ m/^c[0-9]+.*/) { $glocal_consts_refs{$ref} = (); }
	}
    }

    LoadTermTable("$filestem.trmstd",\%greftrmstd,$gstdtrmoffset) if($gdotrmstd > 0);
    LoadTermTable("$filestem.trmnrm",\%greftrmnrm,$gnrmtrmoffset) if($gdotrmnrm > 0);

    $gtargetsnr = $#gnrref;
}

# LoadTermTable($filename,$ref2trm_hash,$offset)
#
# Expects a file created by running fofshared, containing list of 
# shared term codes for references. Loads them into $ref2trm_hash,
# shifting the features numbering by $offset.
sub LoadTermTable
{
    my ($filename,$ref2trm_hash,$offset) = @_;

    open(TRMSTD, "$filename") or die "Cannot read $filename file";
    while($_=<TRMSTD>)
    {
	chop; 
	m/^terms\( *([a-z0-9A-Z_]+) *, *\[(.*)\] *\)\./ 
	    or die "Bad terms info: $_";
	my ($ref, $trms) = ($1, $2);
	my @trms = split(/\ *,/, $trms);
	die "Duplicate reference $ref in $_" if exists $$ref2trm_hash{$ref};
	$$ref2trm_hash{$ref} = [];
	my ($sym);
	foreach $sym (@trms)
	{
	    $sym =~ m/^ *([0-9]+) */ or die "Bad term code $sym in $_";
	    push(@{$$ref2trm_hash{$ref}}, $1 + $offset);
	}
    }
    close TRMSTD;
}


# Create the symbol and reference numbering files
# from the refsyms file. Loads these tables and the refsym table too.
# The initial refsyms file can be created from all (say bushy) problems by running:
# cat */* | bin/GetSymbols -- |sort -u > all.refsyms
sub CreateTables
{
    my $i = 0;
    my ($ref,$sym,$trms,@syms,$psyms,$fsyms);

    open(REFSYMS, "$filestem.refsyms") or die "Cannot read refsyms file";
    open(REFNR, ">$filestem.refnr") or die "Cannot write refnr file";
    open(SYMNR, ">$filestem.symnr") or die "Cannot write symnr file";

    %grefnr = ();
    %gsymnr = ();
    %gsymarity = ();
    %grefsyms = ();
    @gnrsym = ();
    @gnrref = ();

    while($_=<REFSYMS>)
    {
	chop; 
	m/^symbols\( *([a-z0-9A-Z_]+) *, *\[(.*)\] *, *\[(.*)\] *\)\./ 
	    or die "Bad symbols info: $_";
	($ref, $psyms, $fsyms) = ($1, $2, $3);
	my @psyms = split(/\,/, $psyms);
	my @fsyms = split(/\,/, $fsyms);
	die "Duplicate reference $ref in $_" if exists $grefnr{$ref};
	$grefsyms{$ref} = [];
	push(@gnrref, $ref);
	$grefnr{$ref} = $#gnrref;
	print REFNR "$ref\n";

	## this now also remembers arity and symbol kind in %gsymarity
	foreach $sym (@psyms)
	{
	    $sym =~ m/^ *([^\/]+)[\/]([^\/]+)[\/].*/ or die "Bad symbol $sym in $_";
	    $gsymarity{$1} = [$2, 'p'];
	    push(@{$grefsyms{$ref}}, $1);
	}
	foreach $sym (@fsyms)
	{
	    $sym =~ m/^ *([^\/]+)[\/]([^\/]+)[\/].*/ or die "Bad symbol $sym in $_";
	    $gsymarity{$1} = [$2, 'f'];
	    push(@{$grefsyms{$ref}}, $1);
	}

    }
    close REFNR;
    foreach $sym (keys %gsymarity)
    {
	print SYMNR "$sym\n";
	push(@gnrsym, $sym);
	$gsymnr{$sym} = $gsymoffset + $i++;
    }
    close SYMNR;
    close REFSYMS;

    LoadTermTable("$filestem.trmstd",\%greftrmstd,$gstdtrmoffset) if($gdotrmstd > 0);
    LoadTermTable("$filestem.trmnrm",\%greftrmnrm,$gnrmtrmoffset) if($gdotrmnrm > 0);

    $gtargetsnr = $#gnrref;
}

# CreateTables;
# die "finished";
#LoadTables();

sub TestTables
{
 foreach $_ (keys %grefsyms) 
 { print "$_:@{$grefsyms{$_}}\n";}
}

# fields in the %gresults entries
sub res_STATUS  ()  { 0 }
sub res_REFNR   ()  { 1 }
sub res_CPULIM  ()  { 2 }
sub res_REFS    ()  { 3 }  # without the conjecture
sub res_NEEDED  ()  { 4 }  # only for res_STATUS == szs_THEOREM (the needed refs)
                           # and possibly for res_STATUS == szs_COUNTERSAT
                           # (if model was found, it contains its number in @gnrmod -
                           #  this tells to ignore these res_REFS for the countersat precheck,
                           #  because it is subsumed by model precheck; this is slightly imperfect
                           #  now, because a subsuming model can be found later, but I don't care now)

# possible SZS statuses
sub szs_INIT        ()  { 'Initial' } # system was not run on the problem yet
sub szs_UNKNOWN     ()  { 'Unknown' } # used when system dies
sub szs_THEOREM     ()  { 'Theorem' }
sub szs_COUNTERSAT  ()  { 'CounterSatisfiable' }
sub szs_RESOUT      ()  { 'ResourceOut' }
sub szs_GAVEUP      ()  { 'GaveUp' }   # system exited before the time limit for unknown reason


# fields in the @gnrmod entries
# [$file, -1, -1, -1, [], [], []];
# for references just their numbers in @gnrref are used for memory considerations
sub mod_FILE     ()  { 0 }
sub mod_SYMNR    ()  { 1 }
sub mod_POSNR    ()  { 2 }
sub mod_NEGNR    ()  { 3 }
sub mod_SYMS     ()  { 4 }
sub mod_POSREFS  ()  { 5 }
sub mod_NEGREFS  ()  { 6 }

# available ATPs (%gatpdata entries)
# the macros don't work as hash keys (strange)
# sub atp_E     		()  { 'E' }
# sub atp_EP     		()  { 'EP' }
# sub atp_SPASS		()  { 'SPASS' }
# sub atp_VAMPIRE		()  { 'VAMPIRE' }
# sub atp_PARADOX    	()  { 'PARADOX' }
# sub atp_MACE     	()  { 'MACE' }
# sub atp_RANDOCOP  	()  { 'RANDOCOP' }
# sub atp_PROVER9  	()  { 'PROVER' }


# nice names for the table of configurable stuff for ATPs in %gatpdata entries
sub opt_MAXREFS     	()  { 0 }
sub opt_MINREFS		()  { 1 }
sub opt_MINCPU		()  { 2 }
sub opt_MAXCPU		()  { 3 }


## Initialize %gatpdata; 1 billion is used for MAXREFS and -1 serves as uninitialized 
## value for all other params now
## Paradox gets 4 seconds maximal timelimit
## Also normalizes $gspass, $geprover, $gvampire, $gparadox, $gmace to {0,1}
sub InitAtpData
{
    %gatpdata = ();
    @gatpdata{ @gallatps } = ();
    foreach my $atp (keys %gatpdata) { $gatpdata{ $atp } = [1000000000,-1,-1,-1]; }

    $gatpdata{ 'atp_PARADOX' }->[ opt_MAXCPU ] = 4;

    if ($gspass > 1) { $gatpdata{ 'atp_SPASS' }->[ opt_MAXREFS ] = $gspass; $gspass = 1; }
    if ($geprover > 1) { $gatpdata{ 'atp_E' }->[ opt_MAXREFS ] = $geprover; $geprover = 1; }
    if ($gvampire > 1) {$gatpdata{ 'atp_VAMPIRE' }->[ opt_MAXREFS ] = $gvampire; $gvampire = 1;}
    if ($gparadox > 1) {$gatpdata{ 'atp_PARADOX' }->[ opt_MAXREFS ] = $gparadox; $gparadox = 1;}
    if ($gmace > 1) { $gatpdata{ 'atp_MACE' }->[ opt_MAXREFS ] = $gmace; $gmace = 1; }
}

# Following command will create all initial unpruned problem specifications,
# in format spec(name,references), e.g.:
# spec(t119_zfmisc_1,[reflexivity_r1_tarski,t118_zfmisc_1,rc1_xboole_0,dt_k2_zfmisc_1,t1_xboole_1,rc2_xboole_0]).
# and print the into file foo.specs
# for i in `ls */*`; do perl -e   'while(<>) { if(m/^ *fof\( *([^, ]+) *,(.*)/) { ($nm,$rest)=($1,$2); if($rest=~m/^ *conjecture/) {$conjecture=$nm;} else {$h{$nm}=();}}} print "spec($conjecture,[" . join(",", keys %h) . "]).\n";' $i; done >foo.specs

# loads also %gsubrefs and %gsuperrefs if $grefsbgcheat == 1
sub LoadSpecs
{
#    LoadTables();
    %gspec = ();
    %gresults = ();
    %gsubrefs = ();
    %gsuperrefs = ();
    open(SPECS, "$filestem.specs") or die "Cannot read specs file";
    while (<SPECS>) {
	my ($ref,$refs,$ref1);

	m/^spec\( *([a-z0-9A-Z_]+) *, *\[(.*)\] *\)\./ 
	    or die "Bad spec info: $_";

	($ref, $refs) = ($1, $2);
	my @refs = split(/\,/, $refs);
	$gspec{$ref} = {};
	$gresults{$ref} = [];
	my $new_spec = [szs_INIT, $#refs, -1, [@refs], []];
	push(@{$gresults{$ref}}, $new_spec);
	# also some sanity checking
	foreach $ref1 (@refs)
	{
	    exists $grefnr{$ref} or die "Unknown reference $ref in $_";
	    ${$gspec{$ref}}{$ref1} = ();
	}
    }
    close SPECS;
    if ($grefsbgcheat == 1)
    {
	open(SUBREFS, "$filestem.subrefs") or die "Cannot read subrefs file";
	while (<SUBREFS>) {
	    my ($ref,$subrefs,$superrefs,$ref1);

	    m/^refspec\( *([a-z0-9A-Z_]+) *, *\[(.*)\] *, *\[(.*)\] *\)\./ 
		or die "Bad refspec info: $_";

	    ($ref, $subrefs, $superrefs) = ($1, $2, $3);
	    my @subrefs = split(/\, */, $subrefs);
	    my @superrefs = split(/\, */, $superrefs);
	    $gsubrefs{$ref} = {};
	    $gsuperrefs{$ref} = {};
	    # also some sanity checking
	    foreach $ref1 (@subrefs) {
		exists $grefnr{$ref} or die "Unknown reference $ref in $_";
		${$gsubrefs{$ref}}{$ref1} = ();
	    }
	    # also some sanity checking
	    foreach $ref1 (@superrefs) {
		exists $grefnr{$ref} or die "Unknown reference $ref in $_";
		${$gsuperrefs{$ref}}{$ref1} = ();
	    }
	}
    }
}

sub TestSpecs
{
    foreach $_ (keys %gspec) 
    {
	my @refs = keys %{$gspec{$_}};
	print "$_:@refs\n";
    }
}
#LoadSpecs();
#TestSpecs();
#die "finished";

# The initial .to_prove_0 file is just the list of all conjectures in all problems, i.e.:
# cat */*| grep "^ *fof( *[^, ]* *, *conjecture" | sed -e 's/^ *fof( *\([^, ]\+\) *,.*/\1/' > foo.to_prove_0
#
# further iterations are obtained by finding out which conjectures have not been proved yet


# snow is run on the resulting .test_$iter file e.g. this way:
# snow -test -I lear1.test_0 -F lear1.net_0  -L 300 -o allboth  -B :0-1234 
# (it limits the output to 300 most relevant references)

# Print the data for problems on which you want to have advice by the
# machine learner. This takes the file of conjecture names ( .to_prove_$iter ) as input,
# translates the symbols contained in them to numbers, and prints them as testing data to
# file .test_$iter . The number of the conjecture is printed too (it should not influence 
# the testing), in order to make the snow output labeled (better for parsing).
sub PrintTesting
{
    my ($iter) = @_;
    LoadTables();
    open(TO_PROVE, "$filestem.to_prove_$iter") or die "Cannot read to_prove_$iter file";
    open(TEST, ">$filestem.test_$iter") or die "Cannot write test_$iter file";
    while (<TO_PROVE>) {
	chop;
	my $ref = $_;
	exists $grefsyms{$ref} or die "Unknown reference $ref";
	my @syms = @{$grefsyms{$ref}};
	my @syms_nrs   = map { $gsymnr{$_} if(exists($gsymnr{$_})) } @syms;
	if($gdotrmstd > 0)
	{
	    my @trmstd_nrs   = @{$greftrmstd{$ref}};
	    push(@syms_nrs, @trmstd_nrs);
	}
	if($gdotrmnrm > 0)
	{
	    my @trmnrm_nrs   = @{$greftrmnrm{$ref}};
	    push(@syms_nrs, @trmnrm_nrs);
	}
	push(@syms_nrs, $grefnr{$ref});
	my $testing_exmpl = join(",", @syms_nrs);
	print TEST "$testing_exmpl:\n";
    }
    close TO_PROVE;
    close TEST;
}

# also now prints the to_prove_$iter files, which is used as a check
# for SelectRelevantFromSpecs
# the conjecture is printed to become a check for SelectRelevantFromSpecs
sub PrintTestingFromArray
{
    my ($iter,$conjs) = @_;
    my $ref;
    my $iter1 = ($grecadvice > 0) ? $iter . "_" . $grecadvice : $iter;
    open(TO_PROVE,">$filestem.to_prove_$iter") or die "Cannot write to_prove_$iter file";
    open(TEST, ">$filestem.test_$iter1") or die "Cannot write test_$iter1 file";
    foreach $ref (@$conjs) {
	exists $grefsyms{$ref} or die "Unknown reference $ref";
	my @syms = @{$grefsyms{$ref}};
	push(@syms, $gggnewc) if exists $gref2gen{$ref};
	my @syms_nrs   = map { $gsymnr{$_} if(exists($gsymnr{$_})) } @syms;
	if($gdotrmstd > 0)
	{
	    my @trmstd_nrs   = @{$greftrmstd{$ref}};
	    if(exists $gref2gen{$ref})
	    {
		my %tmp = ();
		@tmp{ @trmstd_nrs } = ();
		@tmp{ @{$greftrmstd{$gref2gen{$ref}}} } = ();
		@trmstd_nrs = keys %tmp;
	    }
	    push(@syms_nrs, @trmstd_nrs);
	}
	if($gdotrmnrm > 0)
	{
	    my @trmnrm_nrs   = @{$greftrmnrm{$ref}};
	    if(exists $gref2gen{$ref})
	    {
		my %tmp = ();
		@tmp{ @trmnrm_nrs } = ();
		@tmp{ @{$greftrmnrm{$gref2gen{$ref}}} } = ();
		@trmnrm_nrs= keys %tmp;
	    }
	    push(@syms_nrs, @trmnrm_nrs);
	}
	if(($guseposmodels > 0) && (exists $grefposmods{$ref}))
	{
	    my @posmod_nrs   = map { $gposmodeloffset + $_ } @{$grefposmods{$ref}};
	    push(@syms_nrs, @posmod_nrs);
	}
	if(($gusenegmodels > 0) && (exists $grefnegmods{$ref}))
	{
	    my @negmod_nrs   = map { $gnegmodeloffset + $_ } @{$grefnegmods{$ref}};
	    push(@syms_nrs, @negmod_nrs);
	}
	push(@syms_nrs, $grefnr{$ref});
	my $testing_exmpl = join(",", @syms_nrs);
	print TEST "$testing_exmpl:\n";
	print TO_PROVE "$ref\n";
    }
    close TEST;
    close TO_PROVE;
}

# gets array of specs consisting of conjecture and some axioms
# instead of just a conjecture
# the conjecture is printed to become a check for SelectRelevantFromSpecs
sub PrintTestingFromArrArray
{
    my ($iter,$specs) = @_;
    my $spec1;
    open(TEST, ">$filestem.test_$iter") or die "Cannot write test_$iter file";
    foreach $spec1 (@$specs)
    {
	my @spec = @$spec1;
	my %symsh = ();
	my %trmsh = ();
	my $ref;
	foreach $ref (@spec)
	{
	    exists $grefsyms{$ref} or die "Unknown reference $ref";
	    @symsh{ @{$grefsyms{$ref}} } = ();
	    if($gdotrmstd > 0)
	    {
		my @trmstd_nrs   = @{$greftrmstd{$ref}};
		@trmsh{ @{$greftrmstd{$ref}} } = ();
	    }
	    if($gdotrmnrm > 0)
	    {
		my @trmnrm_nrs   = @{$greftrmnrm{$ref}};
		@trmsh{ @{$greftrmnrm{$ref}} } = ();
	    }
	}
	my @syms_nrs   = map { $gsymnr{$_} if(exists($gsymnr{$_})) } (keys %symsh);
	if(($gdotrmstd > 0) || ($gdotrmnrm > 0))
	{
	    push(@syms_nrs, (keys %trmsh));
	}
	push(@syms_nrs, $grefnr{$spec[0]});
	my $testing_exmpl = join(",", @syms_nrs);
	print TEST "$testing_exmpl:\n";
    }
    close TEST;
}


sub DumpModelInfo
{
    my ($iter) = @_;
    my ($entry,$minfo);
    $iter = "" unless defined $iter;
    my $i = -1;
    open(MODINFO,"| gzip > $filestem.modinfo_$iter.gz");
    foreach $entry (@gnrmod)
    {
	$i++;
	print MODINFO "model($i,['$entry->[mod_FILE]',$entry->[mod_SYMNR],$entry->[mod_POSNR],$entry->[mod_NEGNR],";
	print MODINFO ("[", join(",", @{$entry->[mod_SYMS]}), "],");
	my @posrefs = map { $gnrref[$_] } @{$entry->[mod_POSREFS]};
	my @negrefs = map { $gnrref[$_] } @{$entry->[mod_NEGREFS]};
	print MODINFO ("[", join(",", @posrefs), "],");
	print MODINFO ("[", join(",", @negrefs), "]]).\n");
    }
    close MODINFO;

    open(EQMODS,"| gzip > $filestem.eqmods_$iter.gz");
    foreach $entry (keys %gsum2model)
    {
	my @eqmods = @{$gsum2model{$entry}};
	print EQMODS ("eqmods($#eqmods,[", join(",", @eqmods), "]).\n");
    }
    close EQMODS;

}

sub DumpResults
{
    my ($iter) = @_;
    my ($conj,$result);
    $iter = "" unless defined $iter;
    open(RESULTS,"| gzip > $filestem.results_$iter.gz");
    foreach $conj (sort keys %gresults)
    {
	print RESULTS "results($conj,[";
	my $comma = 0;
	foreach $result (@{$gresults{$conj}})
	{
	    my $ref_str = join(",", @{$result->[res_REFS]});
	    my $needed_str = join(",", @{$result->[res_NEEDED]});
	    if($comma==1) { print RESULTS ",";} else { $comma++; }
	    print RESULTS "res($result->[res_STATUS],$result->[res_REFNR],$result->[res_CPULIM],[$ref_str],[$needed_str])";
	}
	print RESULTS "]).\n";
    }
    close RESULTS;
}

# return hash of conjectures with szs_THEOREM in %gresults
sub GetProvedFromResults
{
    my ($conj,$result);
    my %proved = ();
    foreach $conj (sort keys %gresults)
    {
	foreach $result (@{$gresults{$conj}})
	{
	    if($result->[res_STATUS] eq szs_THEOREM)
	    {
		$proved{$conj} = ();
	    }
	}
    }
    return \%proved;
}


# load %gresults from file; if $load_proved_by == 1, load also the needed slot
# in %gresults from the proved_by files - in that case we do not expect that slot
# to be in the results file
sub LoadResults
{
    my ($filename, $load_proved_by) = @_;
    -r $filename or die "$filename unreadable";
    open(RESULTS,"gzip -dc $filename |") or die "$filename unreadable";
    %gresults = ();
    while($_=<RESULTS>)
    {
	chop;
	m/^results\(([^,]+),\[(.*)\]\)[.]$/ or die "Bad entry in results file: $filename: $_";
	my ($conj,$results_str) = ($1, $2);
#	print "$results_str\n";
	$gresults{$conj} = [];
	if ($load_proved_by == 0)
	{
	    while ($results_str =~ m/res\(([^,]+),([\-0-9]+),([\-0-9]+),\[([^\]]*)\],\[([^\]]*)\]\)/g)
	    {
		my @spec_refs = split(/\,/, $4);
		my @needed_refs = split(/\,/, $5);
		my $new_res = [$1, $2, $3, [@spec_refs], [@needed_refs] ];
		push( @{$gresults{$conj}}, $new_res);
	    }
	}
	else
	{
	    while ($results_str =~ m/res\(([^,]+),([\-0-9]+),([\-0-9]+),\[([^\]]*)\]\)/g)
	    {
		my @spec_refs = split(/\,/, $4);
		my $new_res = [$1, $2, $3, [@spec_refs], [] ];
		push( @{$gresults{$conj}}, $new_res);
	    }
	}
    }
    close RESULTS;

    if ($load_proved_by == 1)
    {
	`cat $filestem.proved_by_* > $filestem.all_proved_by`;
	open(PROVED_BY,"$filestem.all_proved_by");
	while($_=<PROVED_BY>)
	{
	    chop;
	    m/^proved_by\(([^,]+),\[([^\]]*)\]\)\./ or die "Bad proved_by entry: $_";
	    my ($conj,$needed_str) = ($1, $2);
	    (exists $gresults{$conj}) or die "Conjecture not in $filename: $conj in $_";
	    my @conj_entries = @{$gresults{$conj}};
	    my @needed_refs = split(/\,/, $needed_str);
	    ($conj_entries[$#conj_entries]->[res_STATUS] eq szs_THEOREM) or die "Bad last results entry for $conj";
	    $conj_entries[$#conj_entries]->[res_NEEDED] = [ @needed_refs ];
	}
	close PROVED_BY;
    }
}

# testing:
# LoadResults("bl3.results2",0);
# DumpResults();
# exit;


# Include all allowed Mizar refs into $spec
# and delete them from $reserve. All refs with local consts are possibly
# added too if $galwaysmizrefs == 2.
# Modifies $spec and $reserve.
sub AddMizarRefs
{
    my ($spec, $reserve, $all_refs) = @_;
    my $ref1;
    my %mizrefs = ();
    my %specrefs = ();
    @specrefs{ @$spec } = ();
    foreach $ref1 (@$all_refs)
    {
	if(!(exists $specrefs{$ref1})
	   &&
	   (((($galwaysmizrefs == 1) || ($galwaysmizrefs == 3)) && ($ref1 =~ m/$galwaysrefsregexp/)
	     && (!($ref1 =~ m/^t[0-9]+_(numerals|boole|subset|arithm|real)$/)))
	    ||
	    (($galwaysmizrefs == 2) && (exists $glocal_consts_refs{$ref1}))))
	{
	    push(@$spec, $ref1);
	    $mizrefs{$ref1} = ();
	}
    }

    my @newreserve = ();
    foreach $ref1 (@$reserve)
    {
	if(!(exists $mizrefs{$ref1}))
	{
	    push(@newreserve, $ref1);
	}
    }
    @$reserve = @newreserve;
}

# todo: parameterize
sub CombineWSine
{
    my ($iter, $spec1, $reserve1) = @_;

    return ($spec1, $reserve1) unless (($gusesinerel==1) && (0 == $iter % 7) && ($iter > 13));

    my $conj = $spec1->[0];

    if(exists($gsinerel{$conj}))
    {
	my @spec = @$spec1;
	my @reserve  = @$reserve1;
	shift @spec;
	my $cnt1 = $#spec;
	my $cntr = $#reserve;
	my %rh = ();
	my %lh = ();
	my $wr = 0.5;
	my $wl = 1 - $wr;

	my $i = 0;

	foreach my $k (@spec,@reserve) 
	{
	    $rh{$k} = $i * $wr;
	    $i++;
	}

	my $maxr= (2+$cnt1+$cntr) * $wr;

	my $j = 0;

	foreach my $k (@{$gsinerel{$conj}}) 
	{
	    $lh{$k} = $j * $wl;
	    $j++;
	}

	my $maxl = (2+$cnt1+$cntr) * $wl;

	foreach my $k (keys %rh) { if(!(exists $lh{$k})) { $rh{$k} += $maxl }  }
	foreach my $k (keys %lh) { if(exists $rh{$k}) { $rh{$k} += $lh{$k} } else  { $rh{$k} = $maxr + $lh{$k} }  }

	my @res = sort {$rh{$a}<=>$rh{$b}} keys %rh;

	my @newspec = ($conj, @res[0 .. $cnt1 - 1]);
	my @newres  = @res[$cnt1 .. $cnt1 + $cntr];

	return (\@newspec, \@newres);
    }
    else
    {
	print "Warning: no sine info for $conj\n";
	return ($spec1, $reserve1);
    }
}


# Processes one specification suggested by SNoW, and possibly prints
# the task using PrintPruned.
# First field in $spec1 is assumed to be the conjecture here.
# This is done only for unproved entries in %gresults; %gresults
# gets updated with entries with 'Unknown' SZSStatus, and -1 timelimit;
# for each $conj, $gresults{$conj} is an array of arrays
# [SZSStatus,NrOfRefs,TimeLimit,Refs]
# returns 0 if this spec was irrelevant (i.e. already tried before and noted in %gresults),
# and nothing was done, otherwise 1;
# Note that entries in %gspec also contain the conjecture.
# @$spec1 and @$reserve1 are guaranteed to be a subset of @allrefs here.
# ##TODO: improve for lemmatizing
sub HandleSpec
{
    my ($iter, $file_prefix, $file_postfix, $spec3, $reserve3) = @_;

    my ($spec1, $reserve1) = CombineWSine($iter, $spec3, $reserve3);

    my @spec = @$spec1;
    my @reserve = @$reserve1;
    my $conjecture = $spec[0];
    my @all_refs = keys %{$gspec{$conjecture}};

    # include all Mizar refs into @spec if we are told so,
    # and delete them from @reserve
    AddMizarRefs(\@spec, \@reserve, \@all_refs) if ($galwaysmizrefs > 0);

    ## first do srassification - that might avoid addition of more relevant axioms just to
    ## kill countersatisfiability (because the countersatisfiability gets killed by srass);
    ## this modifies @spec and @reserve
    if (($gsrassemul > 0) && ($#reserve >= 0) && (exists $grefnegmods{$conjecture}))
    {
	watch(WSRASS, ("bef_SRASS($conjecture, $iter, [", join(",",@spec), "]).\n"));
	Srassify($conjecture, \@spec, \@reserve);
	watch(WSRASS, ("aft_SRASS($conjecture, $iter, [", join(",",@spec), "]).\n"));
    }

    my $subsumed = 0;
    my $i = 0;

    # for each previous result, check that it does not subsume the
    # new specification; this is now achieved either by being subset of
    # CounterSatisfiable spec, or being equal to any previous spec.
    # If subsumed, try to add one reference from @reserve to @spec and check again -
    # but do this only if $gtimelimit == $mintimelimit not to waste CPU on randomness
    my @results = @{$gresults{$conjecture}};
    while (($i <= $#results) && (0 == $subsumed))
    {
	my $result = $results[$i];
	$i++;

	my @posrefs = ();
	## precompute - this can be taken from the model now
	my ($resrefs, $resrefsnr) = ($result->[res_REFS], $result->[res_REFNR]);
	if(($gcountersatcheck == 2) && (szs_COUNTERSAT eq $result->[res_STATUS]))
	{
	    my @needed = @{$result->[res_NEEDED]};
 	    watch(WCSAT, ("csat1($conjecture, $iter, $i, $#spec, [", join(",",@needed), "]).\n"));
	    if(exists $needed[0])
	    {
		my $model = $gnrmod[$needed[0]];
		@posrefs = map { $gnrref[$_] } @{$model->[mod_POSREFS]};
		$resrefs = \@posrefs;
		$resrefsnr = $model->[mod_POSNR];
		watch(WCSAT, ("csat2($conjecture, $iter, $i, $model, $resrefsnr, '$model->[mod_FILE]', [", join(",",@posrefs), "]).\n"));
	    }
	}

	if(((($#spec <= $resrefsnr) && (szs_COUNTERSAT eq $result->[res_STATUS]))
	    || ($#spec == $resrefsnr)))
	{
	    watch(WCSAT, ("csat3($conjecture, $iter, $i, $resrefsnr, [", join(",",@{$resrefs}), "]).\n"));
	    my %cmp_refs = ();
	    @cmp_refs{ @spec } = ();            # insert the new refs
	    delete @cmp_refs{ @{$resrefs} };   # delete the old ones
	    my @remaining = keys %cmp_refs;
	    if ((-1 == $#remaining) &&
		(($gtimelimit <= $result->[res_CPULIM]) ||
		 (szs_COUNTERSAT eq $result->[res_STATUS]) ||
		 (szs_UNKNOWN eq $result->[res_STATUS])))  # the last one means that systems died on the same input
	    {
		if (($#reserve >= 0) && ($gtimelimit <= $permutetimelimit))
		{
		    my $added = shift @reserve;
		    push(@spec, $added);
		    $i = 0;
		}
		else { $subsumed = 1; }
	    }
	}
    }


    if (0 == $subsumed)
    {
	my $new_spec = [szs_INIT, $#spec, -1, [@spec], [] ];
	push(@{$gresults{$conjecture}}, $new_spec);
	my $new_refs = join(",", @spec);
	print SPEC "spec($conjecture,[$new_refs]).\n";
	PrintPruned($iter, $file_prefix, $file_postfix, \@spec);
	return 1;
    }
    else { return 0; }
}

## can modify $spec; $reserve is now also correctly spliced
sub Srassify
{
    my ($conj, $spec, $reserve) = @_;

    my @spec = @$spec;
    my %neg_conj_mods = ();
    @neg_conj_mods{ @{$grefnegmods{$conj}} } = ();

    my ($ax, $mod);
    foreach $ax (@spec[1 .. $#spec])  ## delete ax's negative models
    {
	delete @neg_conj_mods{ @{$grefnegmods{$ax}} } if (exists $grefnegmods{$ax});
    }

    my $remains = scalar(keys %neg_conj_mods);
    my $i = 0;
    while (($remains > 0) && ($i <= $#{$reserve}))
    {
	my $cand = $reserve->[$i];
	if (exists $grefnegmods{$cand})
	{
	    delete @neg_conj_mods{ @{$grefnegmods{$cand}} };
	    my $tmp = scalar(keys %neg_conj_mods);  ## scalar of () is 0 (not -1)

	    if ($remains > $tmp)
	    {
		push(@$spec, $cand);
		$remains = $tmp;
		splice(@$reserve, $i, 1);
	    }
	}
	$i++;
    }
}


sub PrintPruned
{
    my ($iter, $file_prefix, $file_postfix, $spec) = @_;

    my $conjecture = $spec->[0];
    my $old_file = $file_prefix . $conjecture . $file_postfix;
    (-r $old_file) or die "$old_file not readable!";
#    my $regexp = '"^fof( *\(' . join('\|',@{$spec}) . '\) *,"';
#    `grep $regexp $old_file > $gtmpdir$old_file.s_$iter`;
    open(PRUNED,">$gtmpdir$old_file.s_$iter");
    print PRUNED ('fof(', $conjecture, ',conjecture,', $gref2fla{$conjecture});
    foreach my $i (1 .. $#{$spec})
    {
	print PRUNED ('fof(', $spec->[$i], ',axiom,', $gref2fla{$spec->[$i]});
    }
    close(PRUNED);
}

sub ShakeThreshold
{
    my ($i) = @_;

    if($i==4) { return 6 }
    elsif($i==8) { return 12 }
    elsif($i==16) { return 24 }
    elsif($i==32) { return 40 }
    elsif($i==64) { return 80 }
    elsif($i==128) { return 160 }
    elsif($i==256) { return 320 }
    elsif($i==512) { return 640 }
    else { return $i }
}


# Run SNoW in testing mode on .test_$iter1 and .net_$iter, limiting
# the results to $wantednr; if possible, re-use previous SNoW evaluation.
# Process the results calling HandleSpec.
# Also writes a new spec_$iter file, and cheks test_$iter1 against the to_prove_$iter file.
# The .eval file is no longer written - it goes up to Gigabytes for 
# all of MML
# $newly_proved tells if we have to re-evaluate, or can re-use previous evaluation.
# Note that @spec always contains its conjecture.
# I/O: prints .s_$iter file with spec for each active conjecture;
#      RunProblems() uses that
# Modifies: %gresults
# Returns: list of conjectures to try
sub SelectRelevantFromSpecs
{
    my ($iter, $newly_proved, $threshold, $file_prefix, $file_postfix, $recurse) = @_;

#    LoadSpecs(); # calls LoadTables too

    if(($iter > 20) && ($iter < 30)) { $threshold = ShakeThreshold($threshold); }

    my $previter = $iter - 1;
    my @to_prove = (); # for checking the SNoW output
    open(TO_PROVE, "$filestem.to_prove_$iter") or die "Cannot read to_prove_$iter file";
    while($_=<TO_PROVE>) { chop; push(@to_prove, $_); }
    close TO_PROVE;

    my (@spec, @reserve, %included, $wanted, $check, $act);
    undef $check;
    undef $wanted;
    @spec = ();
    @reserve = ();
    %included = ();
    my @active = ();
    my $do_example = 0;
    my $wantednr = ($glimittargets > 0) ? $glimittargets : $gtargetsnr; # $threshold * 10; # $gtargetsnr;
    my @specs = ();

    ## becomes 0 if no recadvice
    $recurse = $grecadvice unless(defined($recurse));

    if($recurse == 0)
    {
	open(SPEC, ">$filestem.spec_$iter") or die "Cannot write spec_$iter file";
    }

    my $iter1 = ($grecadvice > 0) ? $iter . "_" . $recurse : $iter;

    my $toread = scalar(@to_prove);  ## only used for snowserver
    ## experimental comm through server;
    ## we rely on conjecture being the last number in each testing example
    ## This does not work well because of pipe buffering issues - would have
    ## to be reimplemented using sockets.
    if(($gsnowserver > 0) && ($iter > 3))
    {

	## print initial bogus and set up $gskipsnowres
	foreach my $i (1..$gsnowbogusfactor) { print SNOWWRITER $gsnowbuffbogus; }
	$gskipsnowres += $gsnowbogusfactor;

	open(TEST, "$filestem.test_$iter1");
	my @testlines = <TEST>;
	close(TEST);

	print SNOWWRITER (@testlines);

	$toread = $gskipsnowres + scalar(@testlines);
	## print final bogus; $gskipsnowres will be set up later
	foreach my $i (1..$gsnowbogusfactor) { print SNOWWRITER $gsnowbuffbogus; }
	watch(WSNOW, ('wsn: ', $gskipsnowres, ', ', $toread, "\n"));
    }

    my $previter1 = ($grecadvice > 0) ? $previter . "_" . $recurse : $previter;
    my $evalfile = ($greuseeval > 0) ? "$filestem.eval_$iter1" : "/dev/null";
    my $prevevalfile = "$filestem.eval_$previter1";

    ## run SNoW in testing mode on .test_$iter1 and .net_$iter, limiting
    ## the results to $wantednr; if possible, re-use previous SNoW evaluation
    if(!(($gsnowserver > 0) && ($iter > 3)))
    {
	my $snow_pid = (($greuseeval > 0) &&
			 (NoTesting($iter,$threshold,$newly_proved) > 0)) ?
	    open(SOUT,"gzip -dc $prevevalfile.gz|") :
		open(SOUT,"bin/snow -test -I $filestem.test_$iter1 -F $filestem.net_$iter -L $wantednr -o allboth -B :0-$gtargetsnr|tee $evalfile|");
#	or die("Cannot start snow: $iter1");
    }

    ## process the the evaluations
    my $skipdata = 0;   ## skips training and bogus evals if > 0
    my $exs = 0;
 LINE:
    while ((($gsnowserver > 0) && ($iter > 3) && ($exs <= $toread) && ($_=<SNOWREADER>))
	   || (!(($gsnowserver > 0) && ($iter > 3)) && ($_=<SOUT>)))
    {
	watch(WSNOW, ('sn: ', $_));
	## empty line after the last example was fully read - exit the loop
	last LINE if(($gsnowserver > 0) && ($iter > 3) && ($exs == $toread) && (/^\s*$/));

        if (/^Example/)        # Start entry for a new example
        {
	    $exs++;
	    $skipdata = 0;
	    if($gskipsnowres > 0)
	    {
		$gskipsnowres--;
		$skipdata = 1;
		next LINE;
	    }
	    # print the previous entry
	    if ($do_example == 1)
	    {

		if($recurse > 0)
		{
		    my @spec2 = @spec;
		    push(@specs, \@spec2);
#		    PrintTestingFromArrArray($iter . "_" . ($recurse - 1), \@spec);
		}
		else
		{
		    $act = HandleSpec($iter, $file_prefix, $file_postfix, \@spec, \@reserve);
		    push(@active, $spec[0]) if ($act == 1);
		}
	    }

	    @spec = ();
	    @reserve = ();
	    %included = ();

            /^Example.*: *([0-9]+) */ or die "Bad Example $_ in iter:$iter";

            ($wanted, $check) = ($1, $to_prove[0]);
	    (exists $gnrref[$wanted]) or die "Unknown reference $wanted";
	    if($wanted != $grefnr{$check})
	    {
		if($greuseeval > 0)
		{
		    watch(WREVAL, ('Skipping already proved ', $gnrref[$wanted], ' at ', $check, "\n"));
		    $do_example = 0;
		    $skipdata = 1;
		    next LINE;
		}
		else { die "Not in sync with .to_prove_$iter: $wanted,$gnrref[$wanted],$grefnr{$check},$check"; }
	    }
	    else
	    {
		$do_example = 1;
		push(@spec, shift @to_prove);
	    }
        }
	if (/^([0-9]+):/)
        {

	    next LINE if($skipdata > 0);

	    # Push eligible references - those which are in the initial spec

	    my $refnr = $1;
	    exists $gnrref[$refnr] or die "Parse error - undefined refnr: $_";
	    my $ref0 = $gnrref[$refnr];
	    defined($check) or die "Parse error - undefined example: $_";
	    exists $gspec{$check} or die "Parse error: $check not in gspec: $_";
	    my @tried0 = ();
	    if(!($refnr == $grefnr{$check}))
	    {
		if(exists $ggen2ref{ $ref0 } ) { @tried0 = @{ $ggen2ref{ $ref0 } }; }
		else { @tried0 = ( $ref0 ); }

		foreach my $ref1 (@tried0)
		{
		    if ((exists ${$gspec{$check}}{$ref1})
			&& !(exists $included{$ref1}))
		    {
			if (($#spec < $threshold)
			    || ((($galwaysmizrefs == 1) || ($galwaysmizrefs == 3)) && ($ref1 =~ m/$galwaysrefsregexp/)
				&& (!($ref1 =~ m/^t[0-9]+_(numerals|boole|subset|arithm|real)$/)))
			    || (($galwaysmizrefs == 2) && (exists $glocal_consts_refs{$ref1})))
			{
			    push(@spec, $ref1);
			}
			else { push(@reserve, $ref1); }
			$included{$ref1} = ();
		    }
		}
	    }
	}
    }

    if(($gsnowserver > 0) && ($iter > 3))
    {
	$gskipsnowres = $gsnowbogusfactor;
    }
    else { close(SOUT); }

    # print the last entry
    if ($do_example == 1)
    {
	if($recurse > 0)
	{
	    push(@specs, \@spec);
	    PrintTestingFromArrArray($iter . "_" . ($recurse - 1), \@specs);
	}
	else
	{
	    $act = HandleSpec($iter, $file_prefix, $file_postfix, \@spec, \@reserve);
	    push(@active, $spec[0]) if ($act == 1);
	}
    }

    die "Some entries unhandled in .to_prove_$iter: @to_prove" if ($#to_prove >= 0);
    if($recurse > 0)
    {
	return SelectRelevantFromSpecs($iter,$newly_proved,$threshold, $file_prefix, $file_postfix, $recurse - 1);
    }
    else
    {
	close(SPEC);
	if ($greuseeval > 0)
	{
	    if (NoTesting($iter,$threshold,$newly_proved) > 0)
	    {
		`mv $prevevalfile.gz $evalfile.gz`;
	    }
	    else
	    {
		`gzip $evalfile`;
	    }
	}
	`gzip $filestem.net_$iter` if(!(($gsnowserver > 0) && ($iter > 3)));
	return \@active;
    }
}

# SelectRelevantFromSpecs(0,30,"bushy/",".ren");
# die "finished";


# the algorithm:

# We now assume that all problems are in one flat directory (e.g. "chainy"), and
# that they can be adressed using the name of the conjecture ($conj) and
# common $file_prefix and $file_postfix (so the name is $file_prefix . $conj . $file_postfix).
# There should be no other files in the directory in the beginning.


# 1. create initial specification info (.specs) from the problems by calling
# for i in `ls $file_prefix*$file_postfix`; do perl -e   'while(<>) { if(m/^ *fof\( *([^, ]+) *,(.*)/) { ($nm,$rest)=($1,$2); if($rest=~m/^ *conjecture/) {$conjecture=$nm;} else {$h{$nm}=();}}} print "spec($conjecture,[" . join(",", keys %h) . "]).\n";' $i; done > $filestem.specs


# 2. create the .refsyms table telling for each reference its symbols by calling:
# cat $file_prefix*$file_postfix | bin/GetSymbols -- |sort -u > $filestem.refsyms

# 3. create the numbering files for references and symbols by calling CreateTables

# 4. create the initial .proved_by_0 table (telling that each reference can be proved by itself)
#    from the .refnr file by running:
# sed -e 's/\(.*\)/proved_by(\1,[\1])./' <foo.refnr > foo.proved_by_0

# 5. create the initial SNoW training file .train_0 from the .proved_by_0 file by calling
# PrintTraining(0)

# 6. train SNoW on the initial file (creating the first net file .net_0),
# with references being the targets (i.e. the range is usually 0 - highest reference number (`wc -l foo.refnr`),
#  e,g, this way:
# snow -train -I foo.train_0 -F foo.net_0  -B :0-1234

# 7. create the initial file of conjectures that should be proved (.to_prove_0) from all conjectures in all
# problems (it now holds that every problem contains exactly one conjecture):
# cat */*| grep "^ *fof( *[^, ]* *, *conjecture" | sed -e 's/^ *fof( *\([^, ]\+\) *,.*/\1/' > foo.to_prove_0

# 8. create the initial test file (.test_0) from the initial conjecures to be proved (.to_prove_0);
# to get SNoW hints on them using .net_0: PrintTesting(0);

# 9. evaluate the initial .specs file with the initial net (.net_0), and an initial
# cut-off threshold, on the initial test file (.test_0); say we want only 30 formulas in each file -
# if this is a bushy task, we know that only background formulas should be cut off, but ignore it for now);
# this will create the specs_0 file, and file .s_0 for each prune-able problem:
# snow -test -I lear1.test_0 -F lear1.net_0  -L 300 -o allboth  -B :0-1234 | SelectRelevantFromSpecs(0,30) > lear1.specs_0

# 10. run provers on initial files and .s_0 files to get a new version of the .proved_by table ... probably preceded
#     by creation of a results_0 table, which will keep more info used for avoiding repeating
#     trial of problems; repetitions that should be avoided:
#    - pruned problem was solved
#    - pruned problem was CounterSatisfiable, and newly pruned version is its subset
#    - pruned problem was too hard (timeout), and newly pruned version is 
#        equal to it (note that we should allow supersets, since the previous pruning 
#        might be too drastic, but still too difficult to detect CounterSatisfiability)
#
#  Note that we might also store interesting lemmas as Stephan Schulz's lemmatify does;
#  We might also develop lemmas a la Petr Pudlak, and name them and add them to the learning


# Learn from the .alltrain_$iter file, which was created by PrintTrainingFromHash
# If $newly_proved == -1, and $gusemodels == 0, just move the previous net.
sub Learn
{
    my ($iter, $newly_proved, $threshold) = @_;
    my $next_iter = 1 + $iter;
    print "LEARNING:$iter\n";

    if(($gsnowserver > 0) && ($iter >= 3))
    {
	if($iter > 3)
	{
	    my @trainlines = ();
	    if(-e "$filestem.train_$iter")
	    {
		open(TRAIN1, "$filestem.train_$iter");
		@trainlines = <TRAIN1>;
		close(TRAIN1);
	    }
	    if(($gusemodels > 0) && (-e "$filestem.incrmodels_$iter"))
	    {
		open(MODELS, "$filestem.incrmodels_$iter");
		my @modellines = <MODELS>;
		close(MODELS);
		push (@trainlines, @modellines);
	    }
	    ## we don't care fro the result here
	    watch(WSNOW, ('snow, printing to server training  ', $#trainlines, "examples\n"));
	    if($#trainlines >= 0) { print SNOWWRITER (@trainlines) };
	    watch(WSNOW, ("examples printed to server\n"));
	    $gskipsnowres = $gskipsnowres + scalar(@trainlines);
	}
	elsif($iter == 3)
	{

	    my $wantednr = ($glimittargets > 0) ? $glimittargets : $gtargetsnr; # $threshold * 10; # $gtargetsnr;
	    ## do initial training with alltrain_$iter
	    `bin/snow -train -I $filestem.alltrain_$iter -F $filestem.net_$next_iter  -B :0-$gtargetsnr`;

	    watch(WSNOW, ('starting snow as server with net ', "$filestem.net_$next_iter", "\n"));
	    ## start snow "server"; bogus gets written only on testing
	    $gsnowpid = open2(*SNOWREADER,*SNOWWRITER,"bin/snow -test  -i+ -I /dev/stdin -c- -o allboth -F $filestem.net_$next_iter -L $wantednr -B :0-$gtargetsnr|tee");
	    watch(WSNOW, ('snow started as server with pid ', $gsnowpid, "\n"));
	}
    }

    elsif(NoLearning($iter,$threshold,$newly_proved) > 0)
    {
	`gzip -dc $filestem.net_$iter.gz > $filestem.net_$next_iter`;
    }
    else
    {
	`bin/snow -train -I $filestem.alltrain_$iter -F $filestem.net_$next_iter  -B :0-$gtargetsnr`;
    }
}

sub NoLearning
{
    my ($iter,$threshold,$newly_proved) = @_;

    if(($glearnpolicy == 0) || ($giterpolicy != pol_GROWTH))
    {
	return 1 if(($newly_proved == -1) && ($gusemodels == 0));
	return 0;
    }

    return 0 if(($threshold == $maxthreshold) || ($iter < 4));
    return 1;
}

sub NoTesting
{
    my ($iter,$threshold,$newly_proved) = @_;

    if(($glearnpolicy == 0) || ($giterpolicy != pol_GROWTH))
    {
	return 1 if(($newly_proved == -1) && ($gusemodels == 0));
	return 0;
    }

    return 0 if(($threshold == $minthreshold)
		|| (($gtimelimit > $mintimelimit) && ($threshold == 2 * $minthreshold))
		|| ($iter < 4));
    return 1;
}


# print the models as training examples
# if positive models are also used, print only those that
# are not true in all models (those are very likely just $true)
sub PrintModels
{
    my ($iter) = @_;
    my $tmpref;

    if($gincrmodels > 0)
    {
	## each reference with a found (counter)model(s) gets printed only once,
	open(MODELS, ">$filestem.incrmodels_$iter") or die "Cannot write incrmodels file";
	foreach $tmpref (sort (keys %gitrefposmods, keys %gitrefnegmods))
	{
	    if ((($guseposmodels > 0) && (exists $gitrefposmods{$tmpref})
		 && ($#gnrmod >= scalar( @{$grefposmods{$tmpref}} )))
		|| (($gusenegmodels > 0) && (exists $gitrefnegmods{$tmpref})) )
	    {
		my @syms_nrs = ( $grefnr{$tmpref} );

		if (($guseposmodels > 0) && (exists $gitrefposmods{$tmpref})
		    && ($#gnrmod >= scalar( @{$grefposmods{$tmpref}} )))
		{
		    my @posmod_nrs   = map { $gposmodeloffset + $_ } @{$gitrefposmods{$tmpref}};
		    push(@syms_nrs, @posmod_nrs);
		}
		if (($gusenegmodels > 0) && (exists $gitrefnegmods{$tmpref}))
		{
		    my @negmod_nrs   = map { $gnegmodeloffset + $_ } @{$gitrefnegmods{$tmpref}};
		    push(@syms_nrs, @negmod_nrs);
		}

		my $testing_exmpl = join(",", @syms_nrs);
		print MODELS "$testing_exmpl:\n";
	    }
	}
	close(MODELS);
    }

    open(MODELS, ">$filestem.models_$iter") or die "Cannot write models file";
    foreach $tmpref (sort (keys %grefnr))
    {
	if ((($guseposmodels > 0) && (exists $grefposmods{$tmpref})
	      && ($#gnrmod >= scalar( @{$grefposmods{$tmpref}} )))
	    || (($gusenegmodels > 0) && (exists $grefnegmods{$tmpref})) )
	{
	    my @syms_nrs = ( $grefnr{$tmpref} );

	    if (($guseposmodels > 0) && (exists $grefposmods{$tmpref})
		&& ($#gnrmod >= scalar( @{$grefposmods{$tmpref}} )))
	    {
		my @posmod_nrs   = map { $gposmodeloffset + $_ } @{$grefposmods{$tmpref}};
		push(@syms_nrs, @posmod_nrs);
	    }
	    if (($gusenegmodels > 0) && (exists $grefnegmods{$tmpref}))
	    {
		my @negmod_nrs   = map { $gnegmodeloffset + $_ } @{$grefnegmods{$tmpref}};
		push(@syms_nrs, @negmod_nrs);
	    }

	    my $testing_exmpl = join(",", @syms_nrs);
	    print MODELS "$testing_exmpl:\n";
	}
    }
    close(MODELS);
}



# assumes that $file is a model of last result entry for $conj
sub SetupMaceModel
{
    my ($conj, $file) = @_;
    my @conj_entries = @{$gresults{$conj}};
    my @conj_refs = @{$conj_entries[$#conj_entries]->[res_REFS]};

    my $new_model = [$file, -1, -1, -1, [], [], []];
    push( @gnrmod, $new_model);
    $gmodnr{$file} = $#gnrmod;

    ## find the symbols of the model (up to skolem - is ok)
    my %allowed_syms = ();
    my $tmpref;
    foreach $tmpref ($conj, @conj_refs) {
	@allowed_syms{ @{$grefsyms{$tmpref}} } = ();
    }
    my @allowed_syms = (sort keys %allowed_syms); # input

    $new_model->[mod_SYMNR] = $#allowed_syms;
    $new_model->[mod_SYMS]  = [ @allowed_syms ];

    ## Problem with gmaceemul is that Paradox models sometimes don't
    ## explicitly provide interpretation for irrelevant symbols,
    ## and this kills clausefilter. So we have to forge their
    ## interpretation (otherwise we e.g. might not evaluate some symbols
    ## in the conjecture, and srassemul would suffer).
    if($gmaceemul > 0)
    {
	my %funch = ();
	my %predh = ();
	my %tmp_allowed = %allowed_syms;
	open(MMODEL, "$file.mmodel");
	my @lines = <MMODEL>;
	close(MMODEL);
	my ($funcs0, $preds0) = `grep '^\\(functors\\|predicates\\)(' $file.pout1`;
	$lines[0] =~ m/^interpretation\( *(\d+),.*/ or die "Bad file $file.mmodel";
	my $size = $1;
	$funcs0 =~ m/^functors\(\[(.*)\]\)\./ or die "Bad file $file.pout1";
	my $funcs = $1;
	$preds0 =~ m/^predicates\(\[(.*)\]\)\./ or die "Bad file $file.pout1";
	my $preds = $1;
	my @funcsar = split(/\,/, $funcs);
	my @predsar = split(/\,/, $preds);
	foreach my $f (@funcsar) { $f =~ m/^(.+)\/(\d+)/; $funch{$1} = $2; }
	foreach my $f (@predsar) { $f =~ m/^(.+)\/(\d+)/; $predh{$1} = $2; }
	delete @tmp_allowed{ ('$equal', keys %funch, keys %predh) };

	## fix the model with the default values
	if (scalar(keys %tmp_allowed) > 0)
	{
	    open(MMODEL, ">$file.mmodel");
	    print MMODEL $lines[0];
	    foreach my $s (keys %tmp_allowed)
	    {
		my ($arity, $kind) = @{$gsymarity{$s}};
		my @resarr = (); my $argstr = ""; my $vals = 1;
		foreach (1 .. $arity) { $vals = $vals * $size; }
		foreach (1 .. $vals) { push(@resarr, 0); }
		if($arity > 0)
		{ 
		    my @argarr = ();
		    foreach (1 .. $arity )  { push(@argarr, '_'); }
		    $argstr = '(' . join(',',@argarr) . ')';
		}
		my $kind1 = ($kind eq 'f') ? 'function' : 'relation';
		my $res = $kind1 . '(' . $s . $argstr . ', [' . join(',',@resarr) . '])';
		print MMODEL ($res, ",\n");
	    }
	    print MMODEL (@lines[1 .. $#lines]);
	    close(MMODEL);
	}
    }


    # select references with allowed symbols only
    # this could be done faster by dynamic programming if needed
    my @allowed_refs = ();
    foreach $tmpref (keys %grefnr) {
	my %tmpsymsh = ();
	@tmpsymsh{ @{$grefsyms{$tmpref}} } = ();
	delete @tmpsymsh{ @allowed_syms };
	my @tmp_remaining = keys %tmpsymsh;
	if (-1 == $#tmp_remaining) {
	    push(@allowed_refs, $tmpref);
	}
    }

    # grepping takes 15s (with LANG=C) for 70000 flas; could be done faster if needed
    # no - this was horribly slow (2minutes) and memory eating 700M
    # my $regexp = '"label( *\(' . join('\|',@allowed_refs) . '\))"';

    my @pos_refs = ();
    if($gusemodels > 0)
    {
	open(TMPP9, ">$file.tmpp9");
	foreach $tmpref (@allowed_refs)
	{
	    print TMPP9 ($gref2p9fla{$tmpref}, '# label(', $tmpref, ') # label(axiom).', "\n");
	}
	close (TMPP9);
    }
    if(($guseposmodels > 0) || ($gcountersatcheck ==2))
    {
	open(CLFILT,"bin/clausefilter $file.mmodel true_in_all < $file.tmpp9 | grep label|");
	while($_ = <CLFILT>)
	{
	    if(m/[^#]*# *label\(([^)]*)\).*/)
	    {
		push(@pos_refs, $grefnr{$1});
	    }
	}
	close(CLFILT);
    }

    my @neg_refs = ();
    if($gusenegmodels > 0)
    {
	open(CLFILT,"bin/clausefilter $file.mmodel false_in_all < $file.tmpp9 | grep label|");
	while($_ = <CLFILT>)
	{
	    if(m/[^#]*# *label\(([^)]*)\).*/)
	    {
		push(@neg_refs, $grefnr{$1});
	    }
	}
	close(CLFILT);
    }

    if($gusemodels > 0) { unlink "$file.tmpp9"; }

    # TODO: do not die here, clausefilter can get killed quite easily
    #       just do something sensible with the partial model
    if((($guseposmodels > 0) || ($gcountersatcheck == 2)) && ($gusenegmodels > 0))
    {
	die "bad clausefilter output: $file,:,@allowed_refs,:, @pos_refs,: @neg_refs,:"
	    unless ($#allowed_refs == $#pos_refs + $#neg_refs + 1);
    }

    # memory considerations for 70000 flas
    if(($guseposmodels == 0) && ($gcountersatcheck != 2)) { @pos_refs = (); }
    if($gusenegmodels == 0) { @neg_refs = (); }

    $new_model->[mod_POSNR]   = $#pos_refs;
    $new_model->[mod_NEGNR]   = $#neg_refs;
    $new_model->[mod_POSREFS] = [ @pos_refs ];
    $new_model->[mod_NEGREFS] = [ @neg_refs ];

    foreach $tmpref (@pos_refs)
    {
	push(@{$grefposmods{$gnrref[$tmpref]}}, $#gnrmod);
	push(@{$gitrefposmods{$gnrref[$tmpref]}}, $#gnrmod);
    }
    foreach $tmpref (@neg_refs)
    {
	push(@{$grefnegmods{$gnrref[$tmpref]}}, $#gnrmod);
	push(@{$gitrefnegmods{$gnrref[$tmpref]}}, $#gnrmod);
    }
}

# Create a makefile for this run, and run the problems via make.
# Used for simple parallelization when $gparallelize > 1 .
# Currently only limited to E.
sub RunProblemsFromMakefile
{
    my ($iter, $file_prefix, $file_postfix, $conjs,
	$threshold, $spass, $vampire, $paradox, $keep_cpu_limit) = @_;
    my ($conj,$status,$eprover_status,$spass_status,$vamp_status,$paradox_status,$mace_status);
    my $eprover = $geprover;
    my $mace = $paradox * $gmace;
    my %proved_by = ();
    my ($models_found, $models_old, $models_new) = (0,0,0);

    open(PROVED_BY,">$filestem.proved_by_$iter");
    open(MAKEFILE,">$filestem.Makefile_$iter");
    print MAKEFILE
	("EPROVER = bin/eprover -tAuto -xAuto --tstp-format -s \n",
	 "EPROOF = bin/eproof -tAuto -xAuto --tstp-format -s --cpu-limit=300 \n");
    print MAKEFILE
	("%.out: %\n\t\$(EPROVER) --cpu-limit=$gtimelimit \$* 2>\$*.err | grep \"SZS status\" > \$*.out; true\n",
	 "\tif grep -q Theorem \$*.out; then \$(EPROOF) \$* > \$*.out1; fi; true \n");
    print MAKEFILE "allout: ";
    my $newline = 0;

    foreach $conj (@$conjs)
    {
	my $file = $gtmpdir . $file_prefix . $conj . $file_postfix . ".s_" . $iter;
	my $status = szs_UNKNOWN;
	my @conj_entries = @{$gresults{$conj}};

	($conj_entries[$#conj_entries]->[res_STATUS] eq szs_INIT) 
	    or die "Bad initial results entry for $conj";
	$conj_entries[$#conj_entries]->[res_CPULIM] = $gtimelimit;


	if (($eprover == 1) && 
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{
	    print MAKEFILE "$file.out ";
	    if(++$newline > 500000) { $newline = 0; print MAKEFILE "\\ \n"; }
	}
    }
    print MAKEFILE "\n";
    close(MAKEFILE);

    `make -j $gparallelize -f $filestem.Makefile_$iter`;

    # collect results
    foreach $conj (@$conjs)
    {
	my $file = $gtmpdir . $file_prefix . $conj . $file_postfix . ".s_" . $iter;
	my $status = szs_UNKNOWN;
	my @conj_entries = @{$gresults{$conj}};

	print "$conj: ";


	if (($eprover == 1) && 
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{
	    my $status_line = `grep "SZS status" $file.out`;

	    if ($status_line=~m/.*SZS status[ :]*(.*)/)
	    {
		$status = $1;
	    }
	    else
	    {
		print "Bad status line, assuming szs_UNKNOWN: $file: $status_line";
		$status = szs_UNKNOWN;
	    }
	    print " E: $status";
	    if ($status eq szs_THEOREM)
	    {
		($gtimelimit = $mintimelimit) if ($keep_cpu_limit == 0);
		my $eproof_pid = open(EP,"cat $file.out1| grep ',file('|")
		    or die("bad eproof file $file.out1");
		$proved_by{$conj} = [];
		while ($_=<EP>)
		{
		    m/.*, *file\([^\),]+, *([a-z0-9A-Z_]+) *\)/ or die "bad proof line: $file: $_";
		    my $ref = $1;
		    exists $grefnr{$ref} or die "Unknown reference $ref in $file: $_";
		    push( @{$proved_by{$conj}}, $ref);
		}
	    }
	}

	print "\n";
	$conj_entries[$#conj_entries]->[res_STATUS] = $status;

	if($status eq szs_THEOREM)
	{
	    my $conj_refs = join(",", @{$proved_by{$conj}});
	    print PROVED_BY "proved_by($conj,[$conj_refs]).\n";
	    my %nonconj_refs = ();
	    @nonconj_refs{ @{$proved_by{$conj}} } = ();
	    delete $nonconj_refs{ $conj };
	    $conj_entries[$#conj_entries]->[res_NEEDED] = [ keys %nonconj_refs ];
	}
    }
    close(PROVED_BY);
    DumpResults($iter);
    DumpModelInfo($iter);
    TmpProbIOCleanup($iter, $file_prefix, $file_postfix);
    print "MODELS: found: $models_found, old: $models_old, new: $models_new\n";
    return \%proved_by;
}



# Run prover(s) on problems "$file_prefix$conj$file_postfix.s_$iter".
# Collect the result statuses into %gresults, and if proof was found,
# Collect the axioms used for each proved conjecture to %proved_by and return it.
# Status output is also saved to $file.out, and (possible) proof to $file.out1,
# the proved_by info is logged to proved_by_$iter.
# $spass tells to run SPASS if E fails.
# We try to "exit nicely from here": $gtimelimit is re-set to $mintimelimit whenever a theorem is proved
# - this can cause redundant entries in %gresults - this happens unless $keep_cpu_limit <> 1, which means
# that we are running with high timelimit problems (e.g. when cheating)
# updates the global var:
# ghistory[$iter] = [$nrproved, $nrtried, $threshold, $timelimit, $modelsfound, $proved]

sub RunProblems
{
    my ($iter, $file_prefix, $file_postfix, $conjs,
	$threshold, $spass, $vampire, $paradox, $keep_cpu_limit) = @_;
    my ($conj,$status,$eprover_status,$spass_status,$vamp_status,$paradox_status,$mace_status);
    my $eprover = $geprover;
    my $mace = $paradox * $gmace;
    my %proved_by = ();
    my ($models_found, $models_old, $models_new) = (0,0,0);

    %gitrefposmods = ();
    %gitrefnegmods = ();

    # TODO unhack!
    if(($iter > 20) && ($iter < 30)) { $threshold = ShakeThreshold($threshold); }

#    if($gtimelimit<16) { $spass=1; $vampire=0}
#    if($threshold<16) {$spass=1; $vampire=0}

    if($gmakefile > 0) { return RunProblemsFromMakefile(@_); }


    open(PROVED_BY,">$filestem.proved_by_$iter");
    foreach $conj (@$conjs)
    {
	my $file = $gtmpdir . $file_prefix . $conj . $file_postfix . ".s_" . $iter;
	my $linesnr = `cat $file | wc -l`;
	my $status = szs_UNKNOWN;
	my @conj_entries = @{$gresults{$conj}};
	my $modelnr = -1;

	($conj_entries[$#conj_entries]->[res_STATUS] eq szs_INIT) 
	    or die "Bad initial results entry for $conj";
	$conj_entries[$#conj_entries]->[res_CPULIM] = $gtimelimit;

	print "$conj: ";

	## Paradox is now not bounded from above by gtimelimit, but by
	## $gatpdata{ 'atp_PARADOX' }->[ opt_MAXCPU ] = 4;
	## so the $gtimelimit entry is slightly incorrect in the results table
	if (($paradox == 1) &&
	    ($linesnr <= $gatpdata{ 'atp_PARADOX' }->[ opt_MAXREFS ]) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{
	    my $prdxlimit = min($gtimelimit, $gatpdata{ 'atp_PARADOX' }->[ opt_MAXCPU ]);
	    my $paradox_status_line =
		`bin/$grunner $gcache $prdxlimit bin/paradox --tstp --model --time $prdxlimit $file | tee $file.pout | grep RESULT`;
	    if ($paradox_status_line=~m/CounterSatisfiable/)
	    {
		$paradox_status = szs_COUNTERSAT;
		$status      = szs_COUNTERSAT;

		## forge Mace4 model from Paradox output
		if($gmaceemul == 1)
		{
		    `bin/prdxprep.pl $file.pout > $file.pout1`;
		    `swipl -s bin/prdx2p9.pl -g "prdx2p9('$file.pout1','$file.mmodel'),halt." 2>/dev/null`;
		    $models_found++;
		    my $shasum = `cat $file.mmodel | grep -v interpretation | sha1sum`;
		    if (exists $gsum2model{$shasum})
		    {
			$models_old++;
			$modelnr = $gmodnr{ $gsum2model{$shasum}->[0] };
		    }
		    else
		    {
			SetupMaceModel($conj, $file);
			$modelnr = $#gnrmod;
			$models_new++;
		    }
		    push( @{$gsum2model{$shasum}}, $file);
		}
	    }
	    print " Paradox: $status,";
	}

	## mace is now used to find a model;
	## don't run if paradox was run unsuccesfully
	if (($mace == 1) && (($paradox == 0) || ($status eq szs_COUNTERSAT)) &&
	    ($linesnr <= $gatpdata{ 'atp_MACE' }->[ opt_MAXREFS ]) && ($gtimelimit < 16) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || 
	     ($status eq szs_UNKNOWN) || ($status eq szs_COUNTERSAT)))
	{
	    ## this is buggy due to the bug in tptp_to_ladr/Dec07
	    ## it will make a free var B from ~ ! [B] - this is quite frequent in
	    ## chainy distro
	    my $mace_status_line = 
		`bin/tptp_to_ladr < $file | bin/$grunner $gcache $gtimelimit bin/mace4 -t $gtimelimit | bin/interpformat standard | tee $file.mmodel | grep interpretation`;

	    if ($mace_status_line =~ m/interpretation/)
	    {
		$mace_status = szs_COUNTERSAT;
		$status      = szs_COUNTERSAT;
		$models_found++;

		## find if the model already exists - just by
		## testing if the checksum already exists (really don't care
		## if with probability 10^50 we'll miss a model)
		my $shasum = `cat $file.mmodel | grep -v interpretation | sha1sum`;

		if (exists $gsum2model{$shasum})
		{
		    $models_old++;
		    $modelnr = $gmodnr{ $gsum2model{$shasum}->[0] };
		}
		else
		{
		    SetupMaceModel($conj, $file);
		    $modelnr = $#gnrmod;
		    $models_new++;
		}
		push( @{$gsum2model{$shasum}}, $file);
	    }
	    else { $mace_status = szs_RESOUT; }
	    print " Mace: $mace_status,";
	}


	if (($eprover == 1) && ($grunepar > 0) && ($linesnr <= $gatpdata{ 'atp_E' }->[ opt_MAXREFS ]) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{

	    my $dosine = 0;
	    if($linesnr > 130) { $dosine = 1; }

	    my $status_line = `bin/runepar.pl $gtimelimit $dosine $file | grep "SZS status" |tee $file.out`;
	    if ($status_line=~m/.*SZS status[ :]*(.*)/)
	    {
		$status = $1;
	    }
	    else
	    {
		print "Bad status line, assuming szs_UNKNOWN: $file: $status_line";
		$status = szs_UNKNOWN;
	    }
	    print " E: $status";
	    if ($status eq szs_THEOREM)
	    {

		my $eproof_pid = open(EP,"cat $file.out1 |grep file|") or die("Cannot start grep");
		$proved_by{$conj} = [];
		while ($_=<EP>)
		{
		    m/.*, *file\([^\),]+, *([a-z0-9A-Z_]+) *\)/ or die "bad proof line: $file: $_";
		    my $ref = $1;
		    exists $grefnr{$ref} or die "Unknown reference $ref in $file: $_";
		    push( @{$proved_by{$conj}}, $ref);
		}
		if(($gproblemsfile ne "") && (($gdummy eq "") or !($gdummy=~m/^.*[\/]?$conj$/)))
		{
		    my $origprob = $gltbnames{$conj}->[0];
		    my $solution = $gltbnames{$conj}->[1];
		    `cp $file.out1 $solution`;
		    print "\n% SZS status Theorem for $origprob\n";
		}
	    }
	}


	if (($eprover == 1) && ($grunepar == 0) && ($linesnr <= $gatpdata{ 'atp_E' }->[ opt_MAXREFS ]) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{

	    my $status_line = `bin/$grunner $gcache $gtimelimit bin/eprover -tAuto -xAuto --tstp-format -s --cpu-limit=$gtimelimit $file 2>$file.err | grep "SZS status" |tee $file.out`;

	    if ($status_line=~m/.*SZS status[ :]*(.*)/)
	    {
		$status = $1;
	    }
	    else
	    {
		print "Bad status line, assuming szs_UNKNOWN: $file: $status_line";
		$status = szs_UNKNOWN;
	    }
	    print " E: $status";
	    if ($status eq szs_THEOREM)
	    {
		my $prooftimelimit = $gtimelimit + 60;
		my $eproof_pid = open(EP,"bin/$grunner $gcache $prooftimelimit bin/eproof -tAuto -xAuto --tstp-format $file | tee $file.out1| grep file|")
		    or die("Cannot start eproof");
		$proved_by{$conj} = [];
		while ($_=<EP>)
		{
		    m/.*, *file\([^\),]+, *([a-z0-9A-Z_]+) *\)/ or die "bad proof line: $file: $_";
		    my $ref = $1;
		    exists $grefnr{$ref} or die "Unknown reference $ref in $file: $_";
		    push( @{$proved_by{$conj}}, $ref);
		}
		if(($gproblemsfile ne "") && (($gdummy eq "") or !($gdummy=~m/^.*[\/]?$conj$/)))
		{
		    my $origprob = $gltbnames{$conj}->[0];
		    my $solution = $gltbnames{$conj}->[1];
		    `cp $file.out1 $solution`;
		    print "\n% SZS status Theorem for $origprob\n";
		}
	    }
	}

	if (($spass == 1) && ($linesnr <= $gatpdata{ 'atp_SPASS' }->[ opt_MAXREFS ]) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{
	    my $spass_status_line =
		# `bin/tptp4X -x -f dfg $file | bin/$grunner $gcache $gtimelimit bin/SPASS -Stdin -Memory=900000000 -PGiven=0 -PProblem=0 -TimeLimit=$gtimelimit | grep "SPASS beiseite"| tee $file.outdfg`;
		`bin/$grunner $gcache $gtimelimit bin/SPASS37 -TPTP -Memory=900000000 -PGiven=0 -PProblem=0 -TimeLimit=$gtimelimit $file | grep "SPASS beiseite"| tee $file.outdfg`;

	    if ($spass_status_line=~m/.*SPASS beiseite *: *([^.]+)[.]/)
	    {
		$spass_status = $1;
	    }
	    else
	    {
		print "Bad SPASS status line, assuming szs_UNKNOWN: $file: $spass_status_line";
		$spass_status = szs_UNKNOWN;
	    }

	    if ($spass_status=~m/Proof found/)
	    {
		$spass_status = szs_THEOREM;
		$status= szs_THEOREM;
		my $prooftimelimit = $gtimelimit + 60;
		my $spass_formulae_line = `bin/$grunner $gcache $prooftimelimit bin/SPASS37 -TPTP -Memory=900000000 -PGiven=0 -PProblem=0 -DocProof $file | tee $file.outdfg1| grep "Formulae used in the proof"`;
		($spass_formulae_line=~m/Formulae used in the proof *: *(.*) */) 
		    or die "Bad SPASS Formulae line: $file: $spass_formulae_line";
		my @refs = split(/ +/, $1);
		my $ref;
		foreach $ref (@refs)
		{
		    exists $grefnr{$ref} or die "Unknown reference $ref in $file.outdfg1: $ref";
		}
		$proved_by{$conj} = [@refs];
		if($gtptpproofs > 0)
		{
		    my $regexp = '"^fof( *\(' . join('\|',@refs) . '\) *,"';
		    `grep $regexp $file | bin/eproof -tAuto -xAuto --tstp-format - > $file.out1`;
		    if($gproblemsfile ne "")
		    {
			my $solution = $gltbnames{$conj}->[1];
			`cp $file.out1 $solution`;
		    }
		}
	    }
	    elsif ($spass_status=~m/Completion found/)
	    {
		$spass_status = szs_COUNTERSAT;
		$status= szs_COUNTERSAT;
	    }
	    elsif ($spass_status=~m/Ran out/)
	    {
		$spass_status = szs_RESOUT;
		$status= szs_RESOUT;
	    }
	    print ", SPASS: $spass_status";
	}

	if (($vampire == 1) && ($linesnr <= $gatpdata{ 'atp_VAMPIRE' }->[ opt_MAXREFS ]) &&
	    (($status eq szs_RESOUT) || ($status eq szs_GAVEUP) || ($status eq szs_UNKNOWN)))
	{
	    my $vamp_status_line =
		($gvampire_version eq '9') ? 
		`bin/$grunner $gcache $gtimelimit bin/vampire9 --output_syntax tptp -t $gtimelimit $file 2>$file.errv | tee $file.vout |grep "Refutation"`
		: ($gvampire_version eq '0.6') ?
		`bin/$grunner $gcache $gtimelimit bin/vampire_rel2 -proof tptp -output_axiom_names on --mode casc -t $gtimelimit -m 1234 -input_file $file 2>$file.errv | tee $file.vout |grep "SZS *[sS]tatus *Theorem"`
		: '';

	    if ($vamp_status_line=~m/Refutation|Theorem/)
	    {
		$vamp_status = szs_THEOREM;
		$status      = szs_THEOREM;
		my $vampire_regexp = ($gvampire_version eq '9') ? '.*, *file\([^\),]+, *([a-z0-9A-Z_]+) *\)' :
		    ($gvampire_version eq '0.6') ? '.*\bfile\([^\),]+, *([a-z0-9A-Z_]+) *\)' : '';
		my $vamp_pid = open(VP,"cat $file.vout |grep file|") or die("Cannot start grep");
		
		while ($_=<VP>)
		{
		    # m/.*, *file\([^\),]+, *([a-z0-9A-Z_]+) *\)/ or die "bad proof line: $file: $_";
		    m/$vampire_regexp/ or die "bad proof line: $file: $_";
		    my $ref = $1;
		    exists $grefnr{$ref} or die "Unknown reference $ref in $file.vout: $_";
		    push( @{$proved_by{$conj}}, $ref);
		}
		if($gproblemsfile ne "")
		{
		    my $solution = $gltbnames{$conj}->[1];
		    `cp $file.vout $solution`;
		}
	    }
	    else
	    {
		$vamp_status = szs_RESOUT;
	    }
	    print ", Vampire: $vamp_status";
	}

	print "\n";
	$conj_entries[$#conj_entries]->[res_STATUS] = $status;

	if($status eq szs_THEOREM)
	{
	    ($gtimelimit = $mintimelimit) if ($keep_cpu_limit == 0);
	    # if the proof is empty, add at least the conjecture so
	    # that we do not die here
	    defined($proved_by{$conj}) or push( @{$proved_by{$conj}}, $conj);
	    my $conj_refs = join(",", @{$proved_by{$conj}});
	    print PROVED_BY "proved_by($conj,[$conj_refs]).\n";
	    my %nonconj_refs = ();
	    @nonconj_refs{ @{$proved_by{$conj}} } = ();
	    delete $nonconj_refs{ $conj };
	    $conj_entries[$#conj_entries]->[res_NEEDED] = [ keys %nonconj_refs ];
	}
	elsif(($status eq szs_COUNTERSAT) && ($modelnr > -1))
	{
	    $conj_entries[$#conj_entries]->[res_NEEDED] = [ $modelnr ];
	}

    }
    close(PROVED_BY);
    DumpResults($iter);
    DumpModelInfo($iter);
    TmpProbIOCleanup($iter, $file_prefix, $file_postfix);
    $gitermods[$iter] = $#gnrmod;
    print "MODELS: found: $models_found, old: $models_old, new: $models_new\n";

    my $models = $gitermods[$iter];
    if($iter > 0) { $models = $models - $gitermods[$iter-1]; }

    my %proved = ();   
    @proved{ keys %proved_by } = ();
    $ghistory[$iter] = [scalar keys %proved, scalar @$conjs, $threshold, $gtimelimit, $models, \%proved];

    return \%proved_by;
}

# create $filestem.probio_$iter.tar.gz,
# storing the $iter's problems and their output, and
# deleting them
sub TmpProbIOCleanup
{
    my ($iter, $file_prefix, $file_postfix) = @_;
    my $files = $gtmpdir . $file_prefix . "*" . $file_postfix . ".s_" . $iter;
    my $files1 = $files . ".*" ;
    my $files3 = join(" ", glob($files));
    my $files4 = join(" ", glob($files1));
    my $dir1 = $gtmpdir . $file_prefix;
#    `tar czf $filestem.probio_$iter.tar.gz $files3 $files4`;
    `tar czf $filestem.probio_$iter.tar.gz $dir1`;
    unlink glob($files);
    unlink glob($files1);
}

# GenerateProblemsFromCommonFile($file_prefix, $file_postfix, $common_file);

# Given a $common_file of axioms and conjectures, and $file_prefix
# interpreted as a directory name, create for each conjecture problem file
# named after it (possibly adding $file_postfix), telling to prove
# the conjecture from all axioms in the $common_file.
# 
sub GenerateProblemsFromCommonFile
{
    my ($file_prefix, $file_postfix, $common_file) = @_;
    my ($i,$j);
    die "Remove $file_prefix manually first!" if(-e $file_prefix);
    mkdir($file_prefix);
    my @lAxioms = `bin/tptp4X -t $garitize noint -x -f tptp:short  -u machine -c $common_file | grep "fof.[^,]*,[ ]*axiom"`;
    my @lConjs = `bin/tptp4X -t $garitize noint -x -f tptp:short  -u machine -c $common_file | grep "fof.[^,]*,[ ]*conjecture"`;
    foreach $i (@lConjs)
    {
	$i =~ m/^fof.[ ]*([^ ,]*)[ ]*,/ or die "Bad conjecture name $i";
	my $cname = $1;
	open(PROB,">$file_prefix" . $cname . $file_postfix);
	print PROB @lAxioms;
	print PROB $i;
	close(PROB);
    }
}

# SetupProblemsFromProblemsFile($file_prefix, $problems_file);

# Given a $problems_file, and $file_prefix
# interpreted as a directory name, copy the files from $problems_file
# into $file_prefix, and make note of the relationship in gltbnames;
# If additionally $guniquify > 0, the uniquify.pl script is called to do
# the job, and its result problem and formula mappings are parsed into
# %gltbnames .
sub SetupProblemsFromProblemsFile
{
    my ($file_prefix, $problems_file) = @_;
    die "Remove $file_prefix manually first!" if(-e $file_prefix);
    mkdir($file_prefix);    
    %gltbnames = ();
    if($guniquify > 0)
    {
	`script/uniquify.pl  -f200000 -m0 -c1 $problems_file $file_prefix "$filestem.pmap" "$filestem.fmap"`;
	open(PF,"$filestem.pmap") or die "$filestem.pmap not readable";
	while($_=<PF>)
	{
	    if(m/^\s*(\S+)\s+(\S+)\s*(\S+)\s*$/)
	    {
		my ($file, $pname, $sname) = ($1,$2, $3);
		die "$pname not readable!" unless(-e $file_prefix . $file);
		$gltbnames{$file} = [$pname, $sname];
	    }
	}
	close(PF);
    }
    else
    {
	open(PF,$problems_file) or die "$problems_file not readable";
	while($_=<PF>)
	{
	    if(m/^\s*(\S+)\s+(\S+)\s*$/)
	    {
		my ($pname, $sname) = ($1,$2);
		die "$pname not readable!" unless(-e $pname);
		my ($volume,$directories,$file) = File::Spec->splitpath( $pname );
		die "Non-unique basenames not allowed: $file, $pname, $gltbnames{$file}->[0]"
		    if(exists $gltbnames{$file});
		`cp $pname $file_prefix`;
		$gltbnames{$file} = [$pname, $sname];
	    }
	}
	close(PF);
    }
}


# SetupGeneralization($name,$rest_of_fla)
#
# If $rest_of_fla matches a local constant, it is
# generalized. The generalization is found or created in
# @ggennr2fla and %ggenfla2nr, and the %gref2gen and %ggen2ref
# hashes are updated.
sub SetupGeneralization
{
    my ($nm,$rest) = @_;

    if($rest =~ m/([\(, ])c[0-9]+_*/)
    {
	$rest =~ s/([\(, ])c[0-9]+_[a-zA-Z0-9_]*/$1$gggnewc/g;
	my $gennr;
	if ( exists $ggenfla2nr{$rest} )
	{
	    $gennr = $ggenfla2nr{$rest};
	}
	else
	{
	    push(@ggennr2fla, $rest);
	    $gennr = $#ggennr2fla;
	    $ggenfla2nr{$rest} = $gennr;
	    $gref2fla{'ggennew_' .  $gennr} = $rest . "\n";
	}
	$gref2gen{$nm} = 'ggennew_' .  $gennr;
	push( @{$ggen2ref{'ggennew_' . $gennr}}, $nm);
    }
}


# NormalizeAndCreateInitialSpecs($file_prefix, $file_postfix, $common_file);
#
# Preprocess each file by tptp4X to have no comments, no useful info,
# and one fof per line.
# Create the initial specs file, copy each file to $file.s_0
# Create file containing all formulas (.allflas), 
#   (some of them can occure twice - as axiom and conjecture),
# .allasax - each fla just once (as axiom), .axp9 - the same for prover9/mace4,
# and also all conjecures (.allconjs) and all axioms (.allaxs)
# If ($ggeneralize > 0) formula generalizations are created and set up.
# creates the gsinerel relevances
sub NormalizeAndCreateInitialSpecs
{
    my ($file_prefix, $file_postfix, $common_file, $problems_file) = @_;
    my ($i);

    @ggennr2fla = ();
    %ggenfla2nr = ();
    %gref2gen   = ();
    %ggen2ref   = ();
    %gref2fla   = ();
    %gsinerel   = ();

    if($gtmpdir ne "")
    {
	die "Remove $gtmpdir$file_prefix manually first!" if(-e "$gtmpdir$file_prefix");
	`mkdir $gtmpdir$file_prefix`;
    }
    if($common_file ne "")
    {
	my $last_char = chop($file_prefix);
	die "The --fileprefix option is mandatory and has to end with / when --commonfile used" 
	    if(($file_prefix eq "") or ($last_char ne "/"));
	$file_prefix = $file_prefix . $last_char;
	GenerateProblemsFromCommonFile($file_prefix, $file_postfix, $common_file);
    }
    elsif($problems_file ne "")
    {
	my $last_char = chop($file_prefix);
	die "The --fileprefix option is mandatory and has to end with / when --problemsfile used"
	    if(($file_prefix eq "") or ($last_char ne "/"));
	$file_prefix = $file_prefix . $last_char;
	SetupProblemsFromProblemsFile($file_prefix, $problems_file);
    }
    if($gdummy ne "")
    {
	`cp $gdummy $file_prefix`;
    }
    open(INISPECS,">$filestem.specs");
    my %alllines2 = ();
    foreach $i (glob("$file_prefix*$file_postfix"))
    {
#	chop $i;
	## true axioms formulas are ignored from the very start now, and not included
	## into the normalized problems; saves memory and also crap when boosting is used
	my @lines2 = `bin/tptp4X -t $garitize noint -x -f tptp:short  -u machine -c $i | grep -v '^fof.[^,]*,axiom,[\$\( ]*true[\) ]*[.]'`;
	my $conj = "";
	my %h = ();
	open(PROBLEM,">$i");
	open(PROBLEM1,">$gtmpdir$i.s_0") if($gdofull > 0);
	foreach $_ (@lines2)
	{
	    print PROBLEM $_;
	    print PROBLEM1 $_ if($gdofull > 0);
	    $alllines2{$_} = ();
	    if(m/^ *fof\( *([^, ]+) *, *([^, ]+) *,(.*)/)
	    {
		my ($nm,$status,$rest)=($1,$2,$3);
		if ($status=~m/^conjecture/)
		{
		    $conj=$nm;
		} else {$h{$nm}=();}

		if(! (exists $gref2fla{$nm}))
		{
		    $gref2fla{$nm} = $rest . "\n";
		    SetupGeneralization($nm,$rest) if($ggeneralize > 0);
		}
	    }
	}
	print INISPECS "spec($conj,[" . join(",", keys %h) . "]).\n";
	close(PROBLEM);
	close(PROBLEM1) if($gdofull > 0);

	if($gusesinerel > 0)
	{
	    `bin/e_axfilter -f script/filter1000 $i -o /dev/null`;
	    my $sinefile = $conj . '_pr1000.p';
	    die "Sine failed for $conj" unless(-e $sinefile);
	    $gsinerel{$conj} = [];
	    my $sine_pid = open(SN,"cat $sinefile |") or die("Cannot start cat");
	    while ($_=<SN>)
	    {
		m/^fof.[ ]*([^ ,]*)[ ]*,/ or die "Bad fla name $i";
		push(@{$gsinerel{$conj}}, $1) if ($conj ne $1);
	    }
	    unlink $sinefile;
	}
    }
    close(INISPECS);
    open(ALLFLAS, ">$filestem.allflas");
    foreach $_ (sort keys %alllines2) { print ALLFLAS $_; }
    close(ALLFLAS);
    %alllines2 = ();
    open(ALLGENS, ">$filestem.allgens");
    open(GEN2REF, ">$filestem.gen2ref");
    if($ggeneralize > 0)
    {
	foreach $i (0 .. $#ggennr2fla) 
	{
	    print ALLGENS ("fof(ggennew_$i,axiom,", $ggennr2fla[$i], "\n");
	    print GEN2REF ("gen2ref(ggennew_$i,[", join(",", @{$ggen2ref{'ggennew_' . $i}}), "]).\n");
	}
    }
    close(ALLGENS);
    close(GEN2REF);

    `sed -e 's/,conjecture,/,axiom,/' $filestem.allflas $filestem.allgens | sed -e 's/,lemma,/,axiom,/' | sort -u > $filestem.allasax`;

## proper things for .axp9 is tptp2X -fprover9  $filestem.allasax; followed by:
## perl -e '$/="."; while(<>) { s/% ([^ ]+), axiom[.]/# label(\1) # label(axiom)/; s/[\n]+/ /g; s/[ ]+/ /g; print "$_\n"}' $1 | perl -e '$/="."; while(<>) { s/(#[^\n]*)\n(.*)[.]/\2 \1./; print $_;}'

    if(($gparadox > 0) && (($gmace > 0) || ($gmaceemul > 0)))
    {
	`bin/tptp_to_ladr < $filestem.allasax | grep -v '\(end_of_list\|formulas\|prolog_style\)' > $filestem.axp9`;
	open(AXP9, "$filestem.axp9");
	while($_=<AXP9>)
	{
	    if(m/^(.*)# *label\((.*)\) *# *label\(axiom\).*/)
	    {
		$gref2p9fla{$2} = $1;
	    }
	}
	close(AXP9);
    }
    `grep conjecture $filestem.allflas > $filestem.allconjs`;
    `grep -v conjecture $filestem.allflas > $filestem.allaxs`;
}



sub Iterate
{
    my ($file_prefix, $file_postfix) = @_;
    my ($conj,$i,@tmp_conjs,$to_solve);
    my %conjs_todo = ();
    my $threshold = $maxthreshold;

    # only initialize if we are not recovering
    if($giterrecover == -1)
    {
	# normalize problems and create the initial specs file, copy each file to $file.s_0
	NormalizeAndCreateInitialSpecs($file_prefix, $file_postfix, $gcommonfile, $gproblemsfile);

	# create the refsyms file
	`cat $filestem.allasax | bin/GetSymbols -- |sort -u > $filestem.refsyms`;

	# create the trmstd and trmnrm files
	if (($gdotrmstd > 0) || ($gdostreedefs > 0)) {
	    `cat $filestem.allasax | bin/$gshgenfeatureprg -|sort -u > $filestem.trmstd`;
	}
	if ($gdotrmnrm > 0) {
	    my @lines1 = `cat $filestem.allasax`;
	    my $line;
	    my $fofsh_pid = open(FOFSH,"|bin/$gshgenfeatureprg -|sort -u > $filestem.trmnrm");
	    foreach $line (@lines1) {
		$line =~ s/([\(, ])[A-Z][a-zA-Z0-9_]*/$1A/g;
		print FOFSH $line;
	    }
	    close(FOFSH);
	}
	# create the refnr and symnr files, load these tables and the refsyms table
	CreateTables();
	`cat $file_prefix*.refspec > $filestem.subrefs` if ($grefsbgcheat == 1);
	LoadSpecs();		# initialises %gspec and %gresults
	@conjs_todo{ keys %gspec }  = (); # initialize with all conjectures

	@tmp_conjs = sort keys %conjs_todo;


	# create the initial .proved_by_0 table (telling that each reference can be proved by itself)
	# it gets overwritten by the first RunProblems(), so cat-ing all proved_by_* files
	# together while running still gives all solved problems, but the corresponding
	# .train file remains for further learnings (which is good)
	my %proved_by_0 = ();
	open(PROVED_BY_0,">$filestem.proved_by_0");
	if($gloadprovedby eq "")
	{
	    foreach $i (keys %grefnr)
	    {
		print PROVED_BY_0 "proved_by($i,[$i]).\n";
		push( @{$proved_by_0{$i}}, $i);
	    }
	}
	else
	{
	    # delete spaces in the $gloadprovedby file first
	    my $lpb_pid = open(LOADPROVEDBY, "cat $gloadprovedby | tr -d ' '|");
	    while($_=<LOADPROVEDBY>)
	    {
		chop;
		m/^proved_by\(([^,]+),\[([^\]]*)\]\)\./ or die "Bad proved_by entry: $_";
		my ($conj,$needed_str) = ($1, $2);
		# do not check if conjecture, just if known fla
		# we want to learn from problems that are not being solved now
                # if (exists $gresults{$conj})
		if(exists($grefnr{$conj}))
		{
		    print PROVED_BY_0 "$_\n";
		    my @needed_refs = split(/\,/, $needed_str);
		    my @existing_refs = grep { exists($grefnr{$_}) } @needed_refs;
		    push( @{$proved_by_0{$conj}}, @existing_refs);
		}
		# only warning here, some MPTP files could be removed for various
		# buggyness reasons, and it's a pain to die here
		else { print "Warning: Ignoring unknown conjecture in $gloadprovedby: $conj in $_\n"; }
	    }
	    close(LOADPROVEDBY);
	    # add those unhandled by $gloadprovedby
	    foreach $i (keys %grefnr)
	    {
		if(!(exists $proved_by_0{$i}))
		{
		    print PROVED_BY_0 "proved_by($i,[$i]).\n";
		    push( @{$proved_by_0{$i}}, $i);
		}
	    }
	}
	close(PROVED_BY_0);

	# print the $filestem.train_0 file from .proved_by_0, train on it
	# PrintTraining(0);
	PrintTrainingFromHash(0,\%proved_by_0);
	print "trained 0\n";
	# die "";
	`bin/snow -train -I $filestem.train_0 -F $filestem.net_1  -B :0-$gtargetsnr`;


	# creates the $proved_by_1 hash table, and creates initial .out,out1 files;
	# modifies $gresults! - need to initialize first
	if ($gdofull > 0) {
	    $gtimelimit = ($gdofull == 1) ? $maxtimelimit : $mintimelimit;
	    print "THRESHOLD: 0\nTIMELIMIT: $gtimelimit\n";
	    my $proved_by_1 = RunProblems(0,$file_prefix, $file_postfix,\@tmp_conjs,$threshold,$gspass,$gvampire,$gparadox,1);
	    delete @conjs_todo{ keys %{$proved_by_1}}; # delete the proved ones
	    @tmp_conjs = sort keys %conjs_todo;
	    PrintTrainingFromHash(1,$proved_by_1);
	}
    }
    else
    {
	if($gtmpdir ne "")
	{
	    die "Remove $gtmpdir$file_prefix manually first!" if(-e "$gtmpdir$file_prefix");
	    `mkdir $gtmpdir$file_prefix`;
	}

	LoadTables();
	LoadSpecs();
	LoadResults("$filestem.results_$giterrecover.gz",0);
	my $proved_before = GetProvedFromResults();
	@conjs_todo{ keys %gspec }  = ();            # initialize with all conjectures
	delete @conjs_todo{ keys %{$proved_before}}; # delete the proved ones
	@tmp_conjs = sort keys %conjs_todo;
    }

    if($giterrecover < 1)
    {
	$gtimelimit = $mintimelimit;


	PrintTestingFromArray(1, \@tmp_conjs);    # write testing file for still unproved

	$to_solve = SelectRelevantFromSpecs(1,1,$threshold, $file_prefix, $file_postfix); # write spec_1 file and .s_1 input files

	print "SYMBOL ONLY PASS\n";
	print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
	my $proved_by_2 = RunProblems(1,$file_prefix, $file_postfix,$to_solve,$threshold,$gspass,$gvampire,$gparadox,0);  # creates initial .s_1.out files - omits solved in .proved_by_1
	delete @conjs_todo{ keys %{$proved_by_2}}; # delete the proved ones

	@tmp_conjs = sort keys %conjs_todo;
	PrintTestingFromArray(3,\@tmp_conjs);


	PrintTrainingFromHash(2,$proved_by_2);
    }

    my $iter = $giterrecover;

    if($giterrecover < 3)
    {

	`cat $filestem.train_* > $filestem.alltrain_2`;
	Learn(2,1,$threshold);

	$to_solve = SelectRelevantFromSpecs(3,1,$threshold, $file_prefix, $file_postfix);

	$iter = 3;
    }

    if($giterrecover > 2)
    {

	$gtimelimit = $mintimelimit;
	$threshold = $minthreshold;

	my $previter = $giterrecover - 1;
	`cat $filestem.train_* > $filestem.alltrain_$previter`;
	Learn($previter,1,$threshold);

	$to_solve = SelectRelevantFromSpecs($giterrecover,1,$threshold, $file_prefix, $file_postfix);

	$iter = $giterrecover;
    }


    while ($iter < $gmaxiterlimit)
    {
	my $proved_by = RunProblems($iter,$file_prefix, $file_postfix,$to_solve,$threshold,$gspass,$gvampire,$gparadox,0);
	my @newly_proved = keys %$proved_by;
	# we need a better variating policy here
	if((($iter =~ m/^[258]0$/) || ($iter =~ m/^1[147]0$/)) && ($maxthreshold > 128)) { $maxthreshold = $maxthreshold >> 3; }
	if((($iter =~ m/^[47]0$/) || ($iter =~ m/^1[036]0$/)) && ($maxthreshold < 128)) { $maxthreshold = $maxthreshold << 3; }
	if ($#newly_proved == -1)
	{
	    if ($threshold < $maxthreshold) {
		$threshold = $threshold * 2;
		print "THRESHOLD: $threshold\n";
	    }
	    else
	    {
		# raise the maxtimelimit if we were limited to low runs
		if(($iter > 40) && ($maxtimelimit == 1))
		{
		    $maxtimelimit = 4;
		}
		if(($iter > 60) && ($maxtimelimit == 4))
		{
		    $maxtimelimit = 16;
		}

		if ($gtimelimit < $maxtimelimit)
		{
		    $gtimelimit = 4 * $gtimelimit;
		    $threshold = 2 * $minthreshold; # if timelimit is nonminimal, start with bigger threshold
		    print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
		}
		else
		{
		    if(($giterpolicy == pol_GROWTH) && ($iter < $gminiterlimit))
		    {
			$threshold = $minthreshold;
			$gtimelimit = $mintimelimit;
			print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
		    }
		    else
		    {
			DumpResults(); DumpModelInfo();
			die "reached maximum threshold: $threshold, and timelmit: $gtimelimit";
		    }
		}
	    }
	}
	else # when we learned something new, we restart with $minthreshold and $mintimelimit
	{

	    print "SOLVED: 1+$#newly_proved\n";

	    delete @conjs_todo{ @newly_proved};
	    @tmp_conjs = sort keys %conjs_todo;


	    PrintTrainingFromHash($iter,$proved_by);

	    if ($grefsbgcheat == 1) ## check if we can cheat some bg
	    {
		# this finds all cheatable at once in a fixpoint way - 
		# so there is no need to repeat it here;
		# note that the .train_$iter_cheat_ok and .train_$iter_cheat_fail 
		# files are created too, and
		# .proved_by_$iter_cheat_fail file written with the guys that could not
		# be proved, %gresults is cheated too, to keep info about needed refs for
		# further cheating, but for the loop running properly it is enough
		# to forge just %conjs_todo
		my $cheat_specs = GetCheatableSpecs($iter, \@tmp_conjs, $file_prefix, $file_postfix);
		my @cheated_conjs = keys %{$cheat_specs};
		if ( $#cheated_conjs >= 0)
		{
		    $gtimelimit = $maxtimelimit;
		    print "FOUND CHEATABLE: 1+$#cheated_conjs:\nTIMELIMIT: $gtimelimit\n";

		    $proved_by = RunProblems($iter . "_cheat",$file_prefix, $file_postfix,\@cheated_conjs,$threshold,$gspass,$gvampire,$gparadox,1);

		    PrintTrainingFromHash($iter . "_cheat_ok",$proved_by);


		    @newly_proved = keys %$proved_by;

		    print "SOLVED WITH CHEATING: 1+$#newly_proved\n";

		    delete $cheat_specs->{ @newly_proved };

		    @newly_proved = keys %$cheat_specs;

		    PrintTrainingFromHash($iter . "_cheat_fail",$cheat_specs);

		    open(PROVED_BY,">$filestem.proved_by_$iter" . "_cheat_fail");
		    foreach $conj (sort keys %$cheat_specs)
		    {
			my $conj_refs = join(",", @{$cheat_specs->{$conj}});
			print PROVED_BY "proved_by($conj,[$conj_refs]).\n";
			my @conj_entries = @{$gresults{$conj}};
			$conj_entries[$#conj_entries]->[res_STATUS] = szs_THEOREM;
			$conj_entries[$#conj_entries]->[res_CPULIM] = $gtimelimit;

			my %nonconj_refs = ();
			@nonconj_refs{ @{ $conj_entries[$#conj_entries]->[res_REFS] } } = ();
			delete $nonconj_refs{ $conj };
			$conj_entries[$#conj_entries]->[res_NEEDED] = [ keys %nonconj_refs ];
		    }
		    close(PROVED_BY);

		    delete @conjs_todo{ @newly_proved};
		    @tmp_conjs = sort keys %conjs_todo;

		    print "CHEATED BUT UNSOLVED: 1+$#newly_proved\n";
		}
	    }

	    if (($threshold < $maxthreshold) && ($giterpolicy == pol_GROWTH))
	    {
		$threshold = $threshold * 2;
	    }
	    else { $threshold = $minthreshold; }
	    $gtimelimit = $mintimelimit;
	    print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
	}


	`cat $filestem.train_* > $filestem.alltrain_$iter`;

	if($gusemodels > 0)
	{
	    PrintModels($iter);
	    if($gincrmodels > 0)
	    {
		`cat $filestem.incrmodels_* >> $filestem.alltrain_$iter`;
	    }
	    else { `cat $filestem.models_$iter >> $filestem.alltrain_$iter`; }
	}

	Learn($iter, $#newly_proved,$threshold);
	$iter++;
	PrintTestingFromArray($iter,\@tmp_conjs);
	$to_solve = SelectRelevantFromSpecs($iter, $#newly_proved, $threshold, $file_prefix, $file_postfix);
    }
    DumpResults(); DumpModelInfo();
}

Iterate($gfileprefix,$gfilepostfix);


# return new axiom limit based on the current value
# the basic function
sub StdAxLimitFunc { return $_ * 2; }

my @glowaxlimits = (4,8,12,16,24,32,48,64,80,128);

# hash for the next value and its initialisation
my %gnextlowaxlimit = ();

foreach my $i (0 .. $#glowaxlimits - 1) { $gnextlowaxlimit{$glowaxlimits[$i]} =  $glowaxlimits[$i+1]; }

sub LowAxLimitFunc 
{ 
    if($_ >= $glowaxlimits[$#glowaxlimits])
    {
	return StdAxLimitFunc($_); 
    }
    else
    {
	return $gnextlowaxlimit{$_};
    }
}

my $gaxlimfunc = 'axfun_DBL';

my %gaxlimfuncs = (
    'axfun_DBL' => \&StdAxLimitFunc,
    'axfun_LOW' => \&LowAxLimitFunc
);


# ##TODO: unfinished, unused
# Update the axiom and time limits for the next iteration according
# to the current values and the policy
# global args:
#  ($giterpolicy, $maxthreshold, $minthreshold, $maxtimelimit, $mintimelimit, 
#   $giterlimit, $gmaxiterlimit, $gminiterlimit) 
sub StdLimPol
{
     my ($axlimfunc,$newly_proved,$iter,$gtimelimit,$threshold) = @_; 

     if ($newly_proved == -1)
     {
	 if ($threshold < $maxthreshold) {
	     $threshold = $gaxlimfuncs{$axlimfunc}->($threshold);
	     print "THRESHOLD: $threshold\n";
	 }
	 else
	 {
	     if ($gtimelimit < $maxtimelimit)
	     {
		 $gtimelimit = 4 * $gtimelimit;
		 $threshold = $gaxlimfuncs{$axlimfunc}->($minthreshold); # if timelimit is nonminimal, start with bigger threshold
		 print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
	     }
	     else
	     {
		 if(($giterpolicy == pol_GROWTH) && ($iter < $gminiterlimit))
		 {
		     $threshold = $minthreshold;
		     $gtimelimit = $mintimelimit;
		     print "THRESHOLD: $threshold\nTIMELIMIT: $gtimelimit\n";
		 }
		 else
		 {
		     DumpResults(); DumpModelInfo();
		     die "reached maximum threshold: $threshold, and timelmit: $gtimelimit";
		 }
	     }
	 }
     }
     else
     {
	 if (($threshold < $maxthreshold) && ($giterpolicy == pol_GROWTH))
	 {
	     $threshold = $gaxlimfuncs{$axlimfunc}->($threshold);
	 }
	 else { $threshold = $minthreshold; }
	 $gtimelimit = $mintimelimit;
     }
}

# only create cheatable specs after a run:
# LoadTables();
# $grefsbgcheat = 1;
# LoadSpecs();   # initialises %gspec and %gresults
# LoadResults("bl3.results2",0);
# GetCheatableSpecs(0, $gfileprefix, $gfilepostfix);
# exit;

# Return the hash of cheatable conjectures with their cheated needed references
# (so the same output as from RunProblems() ). 
# the assumption is that everything outside @$conjs is already solved.
# Also sets another %gresults record with szs_INIT for cheatable.
sub GetCheatableSpecs
{
  my ($iter, $file_prefix, $file_postfix, $conjs) = @_;

  my %cheatable_unpr = ();
  my %subr_count = ();
  my ($conj,$ref1,$pr_conj,$unpr_cheat,%all_proved);
  my %cheat_specs = ();

  my $cheat_log = "/dev/null";

  my @cheatable = keys %gsubrefs;

  if(defined $conjs)
  {
      @all_proved{ keys %gspec }  = ();
      delete @all_proved{ @$conjs };
  }
  else # conjectures are all unproved entries in %gresults
  {
      %all_proved = ();
      foreach $conj (keys %gresults)
      {
	  my @conj_entries = @{$gresults{$conj}};
	  if($conj_entries[$#conj_entries]->[res_STATUS] eq szs_THEOREM)
	  {
	      $all_proved{$conj} = ();
	  }
      }
  }

  my @proved_conjs = keys %all_proved;
  my @old_proved = @proved_conjs;
  my @new_proved = ();

  @cheatable_unpr{ @cheatable } = ();
  delete @cheatable_unpr{ @proved_conjs };

  # for each remaining cheatable, set its subref count in %cheatable_unpr
  foreach $conj (keys %cheatable_unpr)
  {

      my @subrefs = keys %{$gsubrefs{ $conj }};
      $cheatable_unpr{ $conj } = 1 + $#subrefs;
  }

  open(CHLOG, ">$cheat_log") or die "Cannot write $cheat_log";
  open(SPEC, ">$filestem.spec_$iter" . "_cheat") or die "Cannot write spec_$iter _cheat file";

  # then decrease the subrefs counts by running through proved conjs
  # set the spec info in %cheat_specs and in %gresults
  while ( $#old_proved >= 0)
  {

      print CHLOG "EXTERNAL: $#old_proved\n";
      my @tmp = keys %cheatable_unpr;
      print CHLOG "$#tmp\n";

      foreach $pr_conj (@old_proved)
      {

	  print CHLOG "LOOP: $pr_conj\n";

	  # only interested in cheatable unproved conjs
	  foreach $unpr_cheat (keys %cheatable_unpr)
	  {
	      print CHLOG "TESTING: $unpr_cheat\n";
#	      print keys %{$gsubrefs{$unpr_cheat}}, "\n";
	      if (($cheatable_unpr{ $unpr_cheat } > 0) &&
		  (exists ${$gsubrefs{$unpr_cheat}}{$pr_conj}))
	      {
		  print CHLOG "$unpr_cheat: $cheatable_unpr{ $unpr_cheat }\n";
		  print CHLOG keys %{$gsubrefs{$unpr_cheat}}, "\n";
		  $cheatable_unpr{ $unpr_cheat } =  $cheatable_unpr{ $unpr_cheat } - 1;
		  print CHLOG "$unpr_cheat: $cheatable_unpr{ $unpr_cheat }\n";
		  if ( 0 == $cheatable_unpr{ $unpr_cheat })
		  {
		      print CHLOG "SUCCESS\n";
		      push(@new_proved, $unpr_cheat);

		      # compute the refernces from %gsubrefs and %gsuperrefs,
		      # update %gresults an %cheat_specs, print SPEC, and the problem file

		      my %refs = ();
		      @refs{ keys %{$gsuperrefs{ $unpr_cheat }} } = ();
		      foreach $conj (keys %{$gsubrefs{ $unpr_cheat }})
		      {
			  # ## ASSERT: last entry for $conj in %gresults is the solved one
			  my @conj_entries = @{$gresults{$conj}};
			  @refs{ @{$conj_entries[$#conj_entries]->[res_NEEDED]} } = ();
		      }

		      # note that there can now be sublevel references in %refs,
		      # irrelevant for $unpr_cheat - we have to filter it with $gspec{$unpr_cheat}
		      # also put the $unpr_cheat to first position in @spec
		      foreach $ref1 (keys %refs)
		      {
			  delete $refs{$ref1} unless exists ${$gspec{$unpr_cheat}}{$ref1};
		      }
		      my @spec = ( $unpr_cheat );
		      push( @spec,  keys %refs);

		      $cheat_specs{$unpr_cheat} = [ @spec ];

		      my $new_spec = [szs_INIT, $#spec, -1, [@spec], [@spec] ];
		      push(@{$gresults{ $unpr_cheat}}, $new_spec);

		      my $new_refs = join(",", @spec);
		      print SPEC "spec($spec[0],[$new_refs]).\n";
		      PrintPruned($iter . "_cheat", $file_prefix, $file_postfix, \@spec);
		  }
	      }
	  }
      }

      @old_proved = @new_proved;
      @new_proved = ();
  }

  close(SPEC);
  close(CHLOG);
  return \%cheat_specs;
}








# structure of the results file:
# results(ConjectureName,OverallSZSStatus,FullSpec,AllowedLemmas,NeededAxioms,OverallTime,
#         [result(IterationNumber,SZSStatus,Spec,Time,[UsefulData]), result(....), ...]).
# where OverallSZSStatus is Unknown, Theorem, CounterSatisfiable (the last should not happen),
# FullSpec is the original list of axioms
# AllowedLemmas are newly invented lemmas logically following from FullSpec, which are therefore
#               eligible for addin to Spec
# NeededAxioms is nonempty only if the OverallSZSStatus is Theorem - then it is
#              a subset of FullSpec and AllowedLemmas
# OverallTime is the time devoted to all proof attempts on this problem
#
# for each result:
# IterationNumber is the iteration duiring whicha result was measured
# SZSStatus is again is the result status (it can be CounterSatisfiable for nonfull spec)
# Spec should be a subset of FullSpec plus AllowedLemmas; 
# Time is the time it took to compute this result
# UsefulData is now empty (may be e.g. other proof characteristics later
#
# memory representation:
# hash( 'conjecture' => ConjectureName,
#       



# When should lemmas generated in problem A be allowed to be used for problem B in bushy and chainy:
# it's simple: whenever they are generated from (a subset of) the axioms of B


# The initial .proved_by_0 table can be created from the .refnr
# file by running:
# sed -e 's/\(.*\)/proved_by(\1,[\1])./' <foo.refnr > foo.proved_by_0
#
# On the result training file .train_0, snow can be run this way (provided
# 1234 is the number of all references, i.e. `wc -l foo.refnr`)
# snow -train -I foo.train_0 -F foo.net_0  -B :0-1234
#
# Further iterations can be obtained e.g. from succesfull proofs, for SPASS e.g. this way:
# grep "Formulae used in the proof" */*.out| sed -e 's/.*__\(.*\).ren.dfg.*: *\(.*\) */proved_by(\1,[\2])./' |tr " " "," > 00zoo



# Create a .train_$iter file from the %proved_by hash, where keys are proved
# conjectures and values are arrays of references needed for the proof.
# All the $filestem.train_* files are afterwards cat-ed to $filestem.alltrain_$iter
# file, on which Learn() works.
sub PrintTrainingFromHash
{
    my ($iter,$proved_by) = @_;
    my $ref;
    open(TRAIN, ">$filestem.train_$iter") or die "Cannot write train_$iter file";
    foreach $ref (sort keys %$proved_by)
    {
	my @refs = @{$proved_by->{$ref}};
	if($ggeneralize > 0)
	{
	    foreach my $rr (@{$proved_by->{$ref}})
	    {
		if(exists $gref2gen{$rr}) { push(@refs, $gref2gen{$rr}); }
	    }
	}
	my @refs_nrs   = map { $grefnr{$_} if(exists($grefnr{$_})) } @refs;
	my @syms = @{$grefsyms{$ref}};
	push(@syms, $gggnewc) if exists $gref2gen{$ref};
	my @syms_nrs   = map { $gsymnr{$_} if(exists($gsymnr{$_})) } @syms;
	my @all_nrs = (@refs_nrs, @syms_nrs);
	if($gdotrmstd > 0)
	{
	    my @trmstd_nrs   = @{$greftrmstd{$ref}};
	    if(exists $gref2gen{$ref})
	    {
		my %tmp = ();
		@tmp{ @trmstd_nrs } = ();
		@tmp{ @{$greftrmstd{$gref2gen{$ref}}} } = ();
		@trmstd_nrs = keys %tmp;
	    }
	    push(@all_nrs, @trmstd_nrs);
	}
	if($gdotrmnrm > 0)
	{
	    my @trmnrm_nrs   = @{$greftrmnrm{$ref}};
	    if(exists $gref2gen{$ref})
	    {
		my %tmp = ();
		@tmp{ @trmnrm_nrs } = ();
		@tmp{ @{$greftrmnrm{$gref2gen{$ref}}} } = ();
		@trmnrm_nrs = keys %tmp;
	    }
	    push(@all_nrs, @trmnrm_nrs);
	}
	if(($guseposmodels > 0) && (exists $grefposmods{$ref}))
	{
	    my @posmod_nrs   = map { $gposmodeloffset + $_ } @{$grefposmods{$ref}};
	    push(@all_nrs, @posmod_nrs);
	}
	if(($gusenegmodels > 0) && (exists $grefnegmods{$ref}))
	{
	    my @negmod_nrs   = map { $gnegmodeloffset + $_ } @{$grefnegmods{$ref}};
	    push(@all_nrs, @negmod_nrs);
	}
	# just a sanity check
	foreach $ref (@refs)
	{
	    exists $grefsyms{$ref} or die "Unknown reference $ref in refs: @refs";
	    exists $grefnr{$ref} or die "Unknown reference $ref in refs: @refs";
	}

	# for 0th iteration, we allow small boost of axioms of
	# small specifications by $gboostweight
	if(($iter == 0) && ($gboostlimit > 0) && (exists $gspec{$ref}))
	{
	    my @all_refs = keys %{$gspec{$ref}};
	    # all_refs contains the conjecture too, so we don't have to add 1 to $#all_refs
	    if($#all_refs <= ($gboostlimit * $gtargetsnr))
	    {
		my @ax_nrs   = map { $grefnr{$_} . '(' . $gboostweight . ')'
					 if(exists($grefnr{$_})) } @all_refs;
		push(@all_nrs, @ax_nrs);
	    }
	}

	my $training_exmpl = join(",", @all_nrs);
	print TRAIN "$training_exmpl:\n";
    }
    close TRAIN;
}


# PrintTesting(0);
#PrintTraining(0);
die "finished";
